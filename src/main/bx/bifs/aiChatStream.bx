/**
 * AI Chat Stream
 */
@BoxBIF
class {

	static final DEFAULT_OPTIONS = {
		timeout : 30,
		logResponse : false,
		logRequest : false
	}
	static MODULE_SETTINGS = getModuleInfo( "bxai" ).settings;

	/**
	 * Inject the following references into the class
	 * - moduleRecord : The ModuleRecord instance
	 * - boxRuntime : The BoxRuntime instance
	 * - interceptorService : The BoxLang InterceptorService
	 * - log : A logger for the module config itself
	 */

    /**
     * Initiate an AI chat stream against the default or custom AI Provider.
	 * <p>
	 * The messages can be a simple string, a struct representing a message or an array of messages or an actual ChatMessage object
	 * <p>
	 * The <code>params</code> are a struct of request params to pass to the provider for the model request according to the provider: Ex	{ temperature: 0.5, max_tokens: 100, model: "gpt-3.5-turbo" }.
	 * You can also use default params in your Module configuration.
	 * <p>
	 * The <code>callback</code> is a function that will be called with each chunk of the stream. The callback function should accept a single argument which is the chunk data.
	 * <p>
	 * The <code>options</code> are a struct of request options to pass to the service for the model request. Available options are:
	 * <ul>
	 * <li>provider:string - The provider to use for the chat. If not passed, we will use the default from the configuration.</li>
	 * <li>apiKey:string - The API Key for the provider. If not passed, we will use the default from the configuration.</li>
	 * <li>timeout:numeric - The timeout in seconds for the request. The default is 30 seconds</li>
	 * <li>logResponse:boolean - Log the response into the ai.log. The default is false</li>
	 * <li>logRequest:boolean - Log the request into the ai.log. The default is false</li>
	 * </ul>
	 *
	 * @messages The messages to pass into the required model. This depends on the provider. It can be a simple string, a struct representing a message or an array of messages or an actual ChatMessage object
	 * @callback A callback function to be called with each chunk of the stream: function( chunk )
	 * @params A struct of request params to pass to the provider for the model request: Ex	{ temperature: 0.5, max_tokens: 100, model: "gpt-3.5-turbo" }
	 * @options A struct of request options to pass to the provider for the model request. Available options are: { provider:string, apiKey:string, timeout:numeric, logResponse:boolean, logRequest:boolean }
	 *
     */
    function invoke(
		required any messages,
		required function callback,
		struct params = {},
		struct options = {}
	) {
		// Incorporate default params with no override
		arguments.params.append( moduleRecord.settings.defaultParams, false );
		arguments.options.append( static.DEFAULT_OPTIONS, false );
		arguments.options.append( static.MODULE_SETTINGS, false );
		arguments.options.stream = true;

		// Build out a chat request
		var oChatRequest = new bxModules.bxai.models.ChatRequest(
			aiMessage( arguments.messages ),
			arguments.params,
			arguments.options
		);

		// Get the provider and chat stream with it
		aiService( provider: arguments.options?.provider ).invokeStream( oChatRequest, arguments.callback );
	}

}
