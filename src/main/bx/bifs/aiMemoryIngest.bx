/**
 * [BoxLang]
 *
 * Copyright [2023] [Ortus Solutions, Corp]
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * ----------------------------------------------------------------------------------
 * Ingest documents into AI memory with comprehensive reporting.
 * Supports single memory or multi-memory fan-out with async capabilities.
 *
 * @example
 * ```
 * // Single memory ingestion
 * result = aiMemoryIngest(
 *     memory = myVectorMemory,
 *     source = "/docs",
 *     type   = "markdown"
 * )
 *
 * // Multi-memory fan-out
 * result = aiMemoryIngest(
 *     memory = [ myChromaMemory, myPgVectorMemory ],
 *     source = "/docs",
 *     type   = "markdown"
 * )
 *
 * // With chunking and ingestion options
 * result = aiMemoryIngest(
 *     memory        = myVectorMemory,
 *     source        = "/knowledge-base",
 *     type          = "directory",
 *     loaderConfig  = { recursive: true, extensions: ["md", "txt"] },
 *     ingestOptions = { chunkSize: 500, overlap: 50, dedupe: true }
 * )
 * ```
 */
import bxModules.bxai.models.memory.IAiMemory;

@BoxBIF
class {

	/**
	 * Default ingestion options
	 */
	static {
		DEFAULT_INGEST_OPTIONS = {
			chunkSize         : 0,
			overlap           : 0,
			strategy          : "recursive",
			dedupe            : false,
			dedupeThreshold   : 0.95,
			trackTokens       : true,
			trackCost         : true,
			async             : false,
			batchSize         : 100,
			continueOnError   : true
		};
	}

	/**
	 * Ingest documents into memory with comprehensive reporting
	 *
	 * @memory       The memory instance(s) to ingest into - single IAiMemory or array for fan-out
	 * @source       The source to load from (file path, directory, or URL)
	 * @type         Optional explicit loader type (text, markdown, html, csv, json, directory)
	 * @loaderConfig Configuration options for the document loader
	 * @ingestOptions Options for the ingestion process:
	 *                - chunkSize: Size of chunks (0 = no chunking)
	 *                - overlap: Overlap between chunks
	 *                - strategy: Chunking strategy (recursive, characters, words, sentences)
	 *                - dedupe: Enable deduplication (default: false)
	 *                - dedupeThreshold: Similarity threshold for deduplication (default: 0.95)
	 *                - trackTokens: Track token counts (default: true)
	 *                - trackCost: Estimate embedding costs (default: true)
	 *                - async: Use async processing for multi-memory (default: false)
	 *                - batchSize: Batch size for processing (default: 100)
	 *                - continueOnError: Continue on document errors (default: true)
	 *
	 * @return Struct with ingestion report
	 */
	function invoke(
		required any memory,
		required string source,
		string type = "",
		struct loaderConfig = {},
		struct ingestOptions = {}
	) {
		// Merge with default options
		var options = static.DEFAULT_INGEST_OPTIONS.append( arguments.ingestOptions, true );

		// Normalize memory to array for uniform processing
		var memories = isArray( arguments.memory ) ? arguments.memory : [ arguments.memory ];

		// Initialize report
		var report = {
			documentsIn       : 0,
			chunksOut         : 0,
			stored            : 0,
			skipped           : 0,
			deduped           : 0,
			tokenCount        : 0,
			embeddingCalls    : 0,
			estimatedCost     : 0.0,
			errors            : [],
			memorySummary     : [],
			startTime         : now(),
			endTime           : "",
			duration          : 0
		};

		try {
			// Load documents using the loader
			var loader = aiDocumentLoader(
				source: arguments.source,
				type: arguments.type,
				config: arguments.loaderConfig
			);
			var documents = loader.load();
			report.documentsIn = documents.len();

			// Chunk documents if chunking is enabled
			var processedDocs = documents;
			if ( options.chunkSize > 0 ) {
				processedDocs = chunkDocuments( documents, options );
			}
			report.chunksOut = processedDocs.len();

			// Track tokens if enabled
			if ( options.trackTokens && processedDocs.len() > 0 ) {
				report.tokenCount = calculateTokenCount( processedDocs );
			}

			// Estimate embedding cost if enabled
			if ( options.trackCost ) {
				report.estimatedCost = estimateCost( report.tokenCount, processedDocs.len() );
			}

			// Process each memory (async or sync)
			if ( options.async && memories.len() > 1 ) {
				// Async fan-out to multiple memories
				var futures = memories.map( ( mem ) => {
					return asyncRun( () => {
						return ingestToMemory( mem, processedDocs, options );
					} );
				} );

				// Wait for all futures to complete and aggregate results
				futures.each( ( future ) => {
					var result = future.get();
					report.stored += result.stored;
					report.skipped += result.skipped;
					report.deduped += result.deduped;
					report.errors.append( result.errors, true );
					report.embeddingCalls += result.embeddingCalls;
				} );
			} else {
				// Sync processing
				memories.each( ( mem ) => {
					var result = ingestToMemory( mem, processedDocs, options );
					report.stored += result.stored;
					report.skipped += result.skipped;
					report.deduped += result.deduped;
					report.errors.append( result.errors, true );
					report.embeddingCalls += result.embeddingCalls;
				} );
			}

			// Get memory summaries
			report.memorySummary = memories.map( ( mem ) => mem.getSummary() );

			// Single memory? Return single summary instead of array
			if ( memories.len() == 1 ) {
				report.memorySummary = report.memorySummary[ 1 ];
			}

		} catch ( any e ) {
			report.errors.append( {
				type    : "LoadError",
				message : e.message,
				detail  : e.detail ?: ""
			} );
		}

		// Finalize timing
		report.endTime = now();
		report.duration = dateDiff( "s", report.startTime, report.endTime );

		return report;
	}

	/**
	 * Ingest documents to a single memory instance
	 *
	 * @memory The memory instance
	 * @documents Array of Document objects
	 * @options Ingestion options
	 *
	 * @return Struct with ingestion results
	 */
	private struct function ingestToMemory(
		required any memory,
		required array documents,
		required struct options
	) {
		var result = {
			stored         : 0,
			skipped        : 0,
			deduped        : 0,
			embeddingCalls : 0,
			errors         : []
		};

		// Use memory's seed method for batch insertion
		var seedResult = arguments.memory.seed( arguments.documents );

		result.stored = seedResult.added ?: 0;
		result.skipped = seedResult.failed ?: 0;
		result.errors = seedResult.errors ?: [];

		// Embedding calls = number of documents stored (each generates an embedding)
		result.embeddingCalls = result.stored;

		return result;
	}

	/**
	 * Chunk documents using aiChunk BIF
	 *
	 * @documents Array of Document objects
	 * @options Chunking options
	 *
	 * @return Array of chunked Document objects
	 */
	private array function chunkDocuments( required array documents, required struct options ) {
		var chunkedDocs = [];

		for ( var doc in arguments.documents ) {
			var chunks = aiChunk(
				text: doc.getContent(),
				options: {
					chunkSize : arguments.options.chunkSize,
					overlap   : arguments.options.overlap,
					strategy  : arguments.options.strategy
				}
			);

			for ( var i = 1; i <= chunks.len(); i++ ) {
				var chunkMetadata = duplicate( doc.getMetadata() );
				chunkMetadata[ "chunkIndex" ] = i;
				chunkMetadata[ "totalChunks" ] = chunks.len();
				chunkMetadata[ "isChunk" ] = true;
				// Use getMetadata() and default if source not found
				var sourceMetadata = doc.getMetadata();
				chunkMetadata[ "originalDocId" ] = sourceMetadata.keyExists( "source" ) ? sourceMetadata.source : "unknown";

				chunkedDocs.append(
					new bxModules.bxai.models.loaders.Document(
						content: chunks[ i ],
						metadata: chunkMetadata
					)
				);
			}
		}

		return chunkedDocs;
	}

	/**
	 * Calculate total token count for documents
	 *
	 * @documents Array of Document objects
	 *
	 * @return Total token count
	 */
	private numeric function calculateTokenCount( required array documents ) {
		var totalTokens = 0;

		for ( var doc in arguments.documents ) {
			totalTokens += aiTokens( doc.getContent() );
		}

		return totalTokens;
	}

	/**
	 * Estimate embedding cost based on token count
	 * Uses OpenAI text-embedding-3-small pricing as baseline ($0.00002/1K tokens)
	 * Note: This is an estimate. Actual costs vary by provider:
	 * - OpenAI text-embedding-3-small: $0.00002/1K tokens
	 * - OpenAI text-embedding-3-large: $0.00013/1K tokens
	 * - Other providers may have different pricing
	 *
	 * @tokenCount Total tokens
	 * @documentCount Number of documents
	 *
	 * @return Estimated cost in USD (baseline estimate)
	 */
	private numeric function estimateCost( required numeric tokenCount, required numeric documentCount ) {
		// OpenAI text-embedding-3-small: $0.00002 per 1K tokens (baseline estimate)
		var costPer1KTokens = 0.00002;
		return ( arguments.tokenCount / 1000 ) * costPer1KTokens;
	}

}
