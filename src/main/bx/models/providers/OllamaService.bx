/**
 * [BoxLang]
 *
 * Copyright [2023] [Ortus Solutions, Corp]
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the
 * License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS"
 * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language
 * governing permissions and limitations under the License.
 * ----------------------------------------------------------------------------------
 */
class extends="BaseService"{

	/**
	 * Static defaults
	 */
	static {
		DEFAULT_CHAT_PARAMS = {
			// Popular local models - users can override with any Ollama model
			"model" : "qwen2.5:0.5b-instruct",
			"stream": false  // Ollama supports both streaming and non-streaming
		}
		DEFAULT_EMBED_PARAMS = {
			"model" : "nomic-embed-text"
		}
		DEFAULT_CHAT_ENDPOINT = "api/chat"
		DEFAULT_EMBED_ENDPOINT = "api/embed"
	}

	/**
	 * Constructor
	 */
	function init(){
		// Default to local Ollama instance
		variables.chatURL = "http://localhost:11434/#static.DEFAULT_CHAT_ENDPOINT#"
		variables.embeddingsURL = "http://localhost:11434/#static.DEFAULT_EMBED_ENDPOINT#"
		variables.name = "Ollama"
		// Ollama typically doesn't require auth headers for local instances
		// But supports basic auth for remote/secured instances
	}

	/**
	 * Configure the service with the API key
	 *
	 * @apiKey - The API key to use with the provider
	 *
	 * @throws InvalidConfiguration if the options are not valid
	 *
	 * @return The configured service instance
	 */
	IAiService function configure( required any options ){
		super.configure( arguments.options )

		// Check if we have an options.baseUrl for all the endpoints
		if( variables.options.keyExists( "baseUrl" ) ){
			var baseUrl = variables.options.baseUrl
			// If it doesn't end with a slash, add it
			if( !baseUrl.endsWith( "/" ) ){
				baseUrl &= "/"
			}
			// Set the endpoints
			variables.chatURL = baseUrl & static.DEFAULT_CHAT_ENDPOINT
			variables.embeddingsURL = baseUrl & static.DEFAULT_EMBED_ENDPOINT
		}

		return this
	}

	/**
	 * Invoke a request to the provider
	 *
	 * @chatRequest The AI request to send to the provider
	 *
	 * @return The response from the provider according to the return format in the AI request
	 */
	function invoke( required AiChatRequest chatRequest ){
		// Seed the chat request with defaults
		arguments.chatRequest.mergeServiceParams( static.DEFAULT_CHAT_PARAMS )
		return super.invoke( arguments.chatRequest )
	}

	/**
	 * Invoke a request to the provider in streaming mode
	 *
	 * @chatRequest The Chat request to send to the provider
	 * @callback A callback function to be called with each chunk of the stream: function( chunk )
	 *
	 * @return void
	 */
	function invokeStream( required AiChatRequest chatRequest, required function callback ){
		// Seed the chat request with defaults
		arguments.chatRequest.mergeServiceParams( static.DEFAULT_CHAT_PARAMS )
		return super.invokeStream( arguments.chatRequest, arguments.callback )
	}

	/**
	 * Ollama chat request - customized for Ollama's API format
	 * @see https://github.com/ollama/ollama/blob/main/docs/api.md
	 *
	 * @chatRequest The AiChatRequest object to send to the provider
	 * @interactionCount The number of interactions (for tool calling recursion tracking)
	 *
	 * @throws ProviderError if the provider returns an error from the request
	 * @throws MaxInteractionsExceeded if tool calls exceed the maximum allowed interactions
	 */
	@override
	public function chat( required AiChatRequest chatRequest, numeric interactionCount = 0 ){
		// Ollama typically doesn't use auth headers for local instances
		// But may use basic auth for remote instances
		if( !arguments.chatRequest.getApiKey().isEmpty() ){
			arguments.chatRequest.addHeader( "Authorization", "Basic #toBase64( arguments.chatRequest.getApiKey() )#" );
		} else {
			arguments.chatRequest.setSendAuthHeader( false );
		}

		// Build the packet according to Ollama's API format
		var dataPacket = {
			"model": arguments.chatRequest.getModel(),
			"messages": arguments.chatRequest.getMessages(),
			"stream": false
		}.append( arguments.chatRequest.getParams() )

		// Add tool support if tools are present
		if( chatRequest.getParams()?.tools?.len() ) {
			dataPacket[ "tools" ] = formatToolsForOllama( chatRequest.getParams().tools )
		}

		// Send it
		var result = sendRequest( arguments.chatRequest, dataPacket )

		// If an error is returned, throw it
		if( result.keyExists( "error" ) ){
			writeLog(
				text: result.error.toString(),
				type: "error",
				log : "ai"
			)
			throw(
				type   : "ProviderError",
				message: result.error.toString()
			);
		}

		// Handle tool use responses
		// Check if Ollama wants to use tools
		var toolCalls = result.message?.tool_calls ?: arrayNew()
		if( toolCalls.len() ){
			/**
			 * ---------------------------------------------------------------------------------------------------------
			 * Max Interactions Check
			 * ---------------------------------------------------------------------------------------------------------
			 */
			var maxAllowed = chatRequest.getMaxInteractions() > 0 ? chatRequest.getMaxInteractions() : variables.maxInteractions;
			if( arguments.interactionCount >= maxAllowed ){
				writeLog(
					text: "Max tool call interactions (#maxAllowed#) exceeded",
					type: "error",
					log : "ai"
				)
				throw(
					type   : "MaxInteractionsExceeded",
					message: "Tool calls exceeded maximum allowed interactions (#maxAllowed#). This may indicate an infinite loop."
				);
			}

			// Add the role assistant message with tool calls
			chatRequest.getMessages().append( result.message );

			// Execute each tool call
			for( var toolCall in toolCalls ) {
				executeOllamaTool( toolCall, chatRequest );
			}

			// Recursively call chat with the updated chat request and incremented interaction count
			return chat( chatRequest, arguments.interactionCount + 1 );
		}

		// Determine return formats
		var content = result.message?.content ?: "";
		switch( arguments.chatRequest.getReturnFormat() ){
			case "all":
				return result;
			case "raw":
				return result;
			case "json":
				return JSONDeserialize( extractFromCodeBlock( content, "json" ) );
			case "xml" :
				return xmlParse( extractFromCodeBlock( content, "xml" ) );
			case  "structuredOutput":
				return populateStructuredOutput( content, arguments.chatRequest );
			case "single": default:
				return content;
		}
	}

	/**
	 * Ollama streaming chat request - overridden for Ollama's streaming format
	 * Ollama uses newline-delimited JSON instead of SSE format
	 *
	 * @chatRequest The AiChatRequestobject to send to the provider
	 * @callback A callback function to be called with each chunk of the stream
	 */
	@override
	public function chatStream( required AiChatRequest chatRequest, required function callback ){
		// Handle authentication for remote/secured Ollama instances
		if( !arguments.chatRequest.getApiKey().isEmpty() ){
			arguments.chatRequest.addHeader( "Authorization", "Basic #toBase64( arguments.chatRequest.getApiKey() )#" );
		} else {
			arguments.chatRequest.setSendAuthHeader( false );
		}

		// Build the packet according to Ollama's API format with streaming
		var dataPacket = {
			"model": arguments.chatRequest.getModel(),
			"messages": arguments.chatRequest.getMessages(),
			"stream": true
		}.append( arguments.chatRequest.getParams() )

		// Add tool support if tools are present
		if( arguments.chatRequest.getParams()?.tools?.len() ) {
			dataPacket[ "tools" ] = formatToolsForOllama( arguments.chatRequest.getParams().tools )
		}

		// Announce the request
		BoxAnnounce(
			"onAIChatRequest",
			{
				"dataPacket" : dataPacket,
				"chatRequest": arguments.chatRequest,
				"provider"   : this
			}
		);

		// Log the request if enabled
		if( arguments.chatRequest.getLogRequestToConsole() ){
			println( "AI Request (Stream)" )
			println( dataPacket )
		}

		// Buffer for accumulating partial lines
		var buffer = "";

		// Capture callback in local variable for closure
		var userCallback = arguments.callback;
		var thisChatRequest = arguments.chatRequest;

		// Build HTTP request with fluent API
		// Note: Ollama uses newline-delimited JSON, NOT SSE format
		var httpRequest = http( getChatURL() )
			.method( "POST" )
			.charset( "utf-8" )
			.timeout( thisChatRequest.getTimeout() )
			.header( "content-type", "application/json" );

		// Auth Header ONLY if the provider requires it
		if( thisChatRequest.getSendAuthHeader() ){
			httpRequest.header( "Authorization", "Bearer #thisChatRequest.getApiKey()#" );
		}

		// Add custom headers
		for( var thisHeader in thisChatRequest.getHeaders() ){
			httpRequest.header( thisHeader, thisChatRequest.getHeaders()[ thisHeader ] );
		}

		// Set request body and callbacks
		httpRequest
			.body( jsonSerialize( dataPacket ) )
			.onChunk( ( chunkNumber, content, totalBytes, httpResult, httpClient, response ) => {
				// For regular streaming (not SSE), onChunk receives:
				// chunkNumber, content (string), totalBytes, httpResult, httpClient, response
				// Ollama sends newline-delimited JSON: {"message":{...}}\n{"message":{...}}\n

				// Append chunk to buffer
				buffer &= content;

				// Process complete lines
				var lines = buffer.split( char(10) );

				// Keep the last potentially incomplete line in the buffer
				buffer = lines[ lines.len() ];

				// Process complete lines (all but the last)
				for( var i = 1; i <= lines.len() - 1; i++ ){
					var line = lines[ i ].trim();

					// Skip empty lines
					if( line.isEmpty() ){
						continue;
					}

					// Each line is a complete JSON object
					try {
						var parsedChunk = jsonDeserialize( line );
						userCallback( parsedChunk );
					} catch( any e ){
						writeLog(
							text: "Error parsing Ollama stream chunk: #e.message# - Data: #line#",
							type: "error",
							log : "ai"
						)
					}
				}
			} )
			.onError( ( error, httpResult ) => {
				var errorMessage = "HTTP Error: #error.message#";

				writeLog(
					text: "Stream Request Error: #errorMessage#",
					type: "error",
					log : "ai"
				)

				throw(
					type   : "ProviderError",
					message: errorMessage
				);
			} )
			.onComplete( ( httpResult ) => {
				// Process any remaining data in buffer
				if( !buffer.isEmpty() ){
					try {
						var parsedChunk = jsonDeserialize( buffer.trim() );
						userCallback( parsedChunk );
					} catch( any e ){
						// Ignore parsing errors for final buffer
					}
				}

				BoxAnnounce(
					"onAIChatResponse",
					{
						"chatRequest" : thisChatRequest,
						"provider"    : this,
						"streamComplete" : true
					}
				);
			} )
			.send();
	}

	/**
	 * Generate embeddings for the given input text(s)
	 *
	 * @embeddingRequest The embedding request object containing input and parameters
	 *
	 * @return The embeddings response from the provider
	 */
	public function embeddings( required AiEmbeddingRequest embeddingRequest ){
		// Seed the embedding request with defaults
		arguments.embeddingRequest
			.setApiKeyIfEmpty( getAPIKey() )
			.mergeServiceParams( static.DEFAULT_EMBED_PARAMS )
			.mergeServiceParams( variables.params )
			.mergeServiceHeaders( variables.headers )

		// Build the packet according to the OpenAI embeddings API standard
		var dataPacket = {
			"input": arguments.embeddingRequest.getInput(),
			"model": arguments.embeddingRequest.getModel()
		}.append( arguments.embeddingRequest.getParams() )

		// Send it
		var result = sendEmbeddingRequest( embeddingRequest, dataPacket )

		// If an error is returned, throw it
		if( result.keyExists( "error" ) ){
			writeLog(
				text: result.error.toString(),
				type: "error",
				log : "ai"
			)

			// Announce the error
			BoxAnnounce( "onAIError", {
				error: result.error,
				errorMessage: result.error.toString(),
				provider: this,
				operation: "embeddings",
				embeddingRequest: arguments.embeddingRequest,
				canRetry: true
			} );

			throw(
				type   : "ProviderError",
				message: result.error.toString()
			);
		}

        // Verify that data key exists, else throw an exception with the raw response
        if( !result.keyExists( "embeddings" ) ){
            writeLog(
                text: "Invalid embedding response: #result.toString()#",
                type: "error",
                log : "ai"
            )

            // Announce the error
            BoxAnnounce( "onAIError", {
                error: "Invalid embedding response",
                errorMessage: "Invalid embedding response from provider. Response: #result.toString()#",
                provider: this,
                operation: "embeddings",
                embeddingRequest: arguments.embeddingRequest,
                canRetry: false
            } );

            throw(
                type   : "ProviderError",
                message: "Invalid embedding response from provider. Response: #result.toString()#"
            );
        }

		// Determine return formats
		switch( embeddingRequest.getReturnFormat() ){
			case "embeddings":
				return result.embeddings.map( item => item );
			case "raw":
				return result;
			case "first": default:
				return result.embeddings.first();
		}
	}

	/**
	 * Format tools for Ollama's specific tool format
	 * @see https://github.com/ollama/ollama/blob/main/docs/api.md#tool-calling
	 *
	 * @tools The tools to format
	 *
	 * @return An array of tools formatted for Ollama
	 */
	private array function formatToolsForOllama( required array tools ) {
		var ollamaTools = []

		for( var tool in arguments.tools ) {
			var argumentsSchema = tool.getArgumentsSchema();
			var ollamaTool = {
				"type": "function",
				"function": {
					"name": tool.getName(),
					"description": tool.getDescription(),
					"parameters": {
						"type": "object",
						"properties": argumentsSchema.properties,
						"required": argumentsSchema.required
					}
				}
			}
			ollamaTools.append( ollamaTool )
		}

		return ollamaTools
	}

	/**
	 * Execute Ollama tool calls
	 *
	 * @toolCall The tool call object from Ollama: { function: { name: "toolName", arguments: {} } }
	 * @chatRequest The original chat request containing the tool
	 */
	private function executeOllamaTool( required struct toolCall, required AiChatRequest chatRequest ) {
		var messages = arguments.chatRequest.getMessages();

		// Get the tool from the chat request
		arguments.chatRequest
			.getTool( toolCall.function.name )
			.ifPresentOrElse(
				// Found, invoke the tool
				tool => {
					messages.append({
						"role" : "tool",
						"content" : tool.invoke( args : toolCall.function.arguments )
					});
				},
				// Not found
				() => {
					writeLog(
						text: "Unable to find tool named: [#toolCall.function.name#]",
						type: "warning",
						log : "ai"
					)
					messages.append({
						"role" : "tool",
						"content" : "Tool ['#toolCall.function.name#'] not found in chat request"
					});
				}
			);
	}

}