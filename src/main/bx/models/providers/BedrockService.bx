/**
 * [BoxLang]
 *
 * Copyright [2023] [Ortus Solutions, Corp]
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the
 * License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS"
 * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language
 * governing permissions and limitations under the License.
 * ----------------------------------------------------------------------------------
 * AWS Bedrock Service Provider
 *
 * This provider enables integration with AWS Bedrock for accessing foundation models
 * like Claude, Titan, Llama, and others through AWS's managed service.
 *
 * Unlike other providers that use API keys, Bedrock uses AWS IAM authentication
 * with Signature Version 4 request signing.
 *
 * Supported model families:
 * - Anthropic Claude (claude-3-*, claude-instant-*)
 * - Amazon Titan (amazon.titan-*)
 * - Meta Llama (meta.llama-*)
 * - Cohere (cohere.*)
 * - AI21 Labs (ai21.*)
 * - Mistral (mistral.*)
 *
 * @see https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html
 * @see https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html
 */
import bxModules.bxai.models.util.AwsSignatureV4;

class extends = "BaseService"{
	/**
	 * AWS Region for Bedrock
	 */
	property name="region" default="us-east-1";

	/**
	 * AWS Access Key ID (optional - can use environment variables or IAM role)
	 */
	property name="awsAccessKeyId" default="";

	/**
	 * AWS Secret Access Key (optional - can use environment variables or IAM role)
	 */
	property name="awsSecretAccessKey" default="";

	/**
	 * AWS Session Token (optional - for temporary credentials)
	 */
	property name="awsSessionToken" default="";

	/**
	 * The model ID for Bedrock (e.g., anthropic.claude-3-sonnet-20240229-v1:0)
	 */
	property name="modelId" default="";

	/**
	 * Static defaults
	 */
	static {
		DEFAULT_CHAT_PARAMS = {
			"model"      : "anthropic.claude-3-sonnet-20240229-v1:0",
			"max_tokens" : 4096
		};
		// Bedrock service name for AWS signing
		AWS_SERVICE = "bedrock";
	}

	/**
	 * Constructor
	 */
	function init(){
		variables.name          = "Bedrock";
		// Bedrock doesn't use a fixed URL - it's constructed per request based on region and model
		variables.chatURL       = "";
		variables.embeddingsURL = "";

		// Set default params
		defaults( static.DEFAULT_CHAT_PARAMS );

		// Load AWS credentials from environment if not provided
		loadAwsCredentialsFromEnvironment();

		return this;
	}

	/**
	 * Configure the service with AWS credentials and region
	 *
	 *         OR a simple string for backward compatibility (treated as modelId)
	 *
	 * @config A struct containing: region, awsAccessKeyId, awsSecretAccessKey, awsSessionToken (optional), modelId (optional)
	 *
	 * @return The service instance
	 */
	IAiService function configure( required any config ){
		if ( isSimpleValue( arguments.config ) ) {
			// If a simple string is passed, treat it as model ID for compatibility
			variables.modelId = arguments.config;
		} else if ( isStruct( arguments.config ) ) {
			// Check if credentials are nested inside apiKey (from aiChat/aiService flow)
			var credSource = arguments.config;
			if ( arguments.config.keyExists( "apiKey" ) && isStruct( arguments.config.apiKey ) ) {
				credSource = arguments.config.apiKey;
			}

			// Configure with struct of settings - check both top-level and apiKey nested
			if ( credSource.keyExists( "region" ) ) {
				variables.region = credSource.region;
			}
			if ( credSource.keyExists( "awsAccessKeyId" ) ) {
				variables.awsAccessKeyId = credSource.awsAccessKeyId;
			}
			if ( credSource.keyExists( "awsSecretAccessKey" ) ) {
				variables.awsSecretAccessKey = credSource.awsSecretAccessKey;
			}
			if ( credSource.keyExists( "awsSessionToken" ) ) {
				variables.awsSessionToken = credSource.awsSessionToken;
			}
			// These remain at top level
			if ( arguments.config.keyExists( "modelId" ) ) {
				variables.modelId = arguments.config.modelId;
			}
			if ( arguments.config.keyExists( "model" ) ) {
				variables.params.model = arguments.config.model;
			}
		}

		return this;
	}

	/**
	 * Load AWS credentials from environment variables if not already set
	 */
	private function loadAwsCredentialsFromEnvironment(){
		if ( !len( variables.awsAccessKeyId ) ) {
			variables.awsAccessKeyId = getSystemSetting( "AWS_ACCESS_KEY_ID", "" );
		}
		if ( !len( variables.awsSecretAccessKey ) ) {
			variables.awsSecretAccessKey = getSystemSetting( "AWS_SECRET_ACCESS_KEY", "" );
		}
		if ( !len( variables.awsSessionToken ) ) {
			variables.awsSessionToken = getSystemSetting( "AWS_SESSION_TOKEN", "" );
		}
		if ( !len( variables.region ) || variables.region == "us-east-1" ) {
			var envRegion = getSystemSetting( "AWS_REGION", getSystemSetting( "AWS_DEFAULT_REGION", "" ) );
			if ( len( envRegion ) ) {
				variables.region = envRegion;
			}
		}
	}

	/**
	 * Get the Bedrock endpoint URL for the configured region and model
	 *
	 * @modelIdOrArn The model ID or inference profile ARN to invoke
	 *
	 * @return The full endpoint URL
	 */
	private string function getBedrockEndpoint( required string modelIdOrArn ){
		return "https://bedrock-runtime.#variables.region#.amazonaws.com#getBedrockPath( arguments.modelIdOrArn )#";
	}

	/**
	 * Get the Bedrock host for the configured region
	 */
	private string function getBedrockHost(){
		return "bedrock-runtime.#variables.region#.amazonaws.com";
	}

	/**
	 * Get the request path for a model invocation
	 * Supports both model IDs and inference profile ARNs
	 *
	 * @modelIdOrArn The model ID or inference profile ARN to invoke
	 */
	private string function getBedrockPath( required string modelIdOrArn ){
		// Check if this is an inference profile ARN
		if ( isInferenceProfileArn( arguments.modelIdOrArn ) ) {
			return "/application-inference-profile/#arguments.modelIdOrArn#/invoke";
		}
		return "/model/#arguments.modelIdOrArn#/invoke";
	}

	/**
	 * Get the Bedrock streaming endpoint URL for the configured region and model
	 *
	 * @modelIdOrArn The model ID or inference profile ARN to invoke
	 *
	 * @return The full streaming endpoint URL
	 */
	private string function getBedrockStreamEndpoint( required string modelIdOrArn ){
		return "https://bedrock-runtime.#variables.region#.amazonaws.com#getBedrockStreamPath( arguments.modelIdOrArn )#";
	}

	/**
	 * Get the request path for a streaming model invocation
	 * Supports both model IDs and inference profile ARNs
	 *
	 * @modelIdOrArn The model ID or inference profile ARN to invoke
	 */
	private string function getBedrockStreamPath( required string modelIdOrArn ){
		// Check if this is an inference profile ARN
		if ( isInferenceProfileArn( arguments.modelIdOrArn ) ) {
			return "/application-inference-profile/#arguments.modelIdOrArn#/invoke-with-response-stream";
		}
		return "/model/#arguments.modelIdOrArn#/invoke-with-response-stream";
	}

	/**
	 * Check if the given string is an AWS Bedrock inference profile ARN
	 *
	 * @arnOrId The string to check
	 *
	 * @return True if this is an inference profile ARN
	 */
	private boolean function isInferenceProfileArn( required string arnOrId ){
		return arguments.arnOrId.findNoCase( "arn:aws:bedrock" ) > 0
			&& arguments.arnOrId.findNoCase( "inference-profile" ) > 0;
	}

	/**
	 * Detect the model family from the model ID
	 *
	 * @modelId The Bedrock model ID
	 *
	 * @return The model family: "claude", "titan", "llama", "cohere", "ai21", "mistral", "openai", or "unknown"
	 */
	private string function detectModelFamily( required string modelId ){
		var id = arguments.modelId.lcase();

		if ( id.findNoCase( "anthropic" ) || id.findNoCase( "claude" ) ) {
			return "claude";
		} else if ( id.findNoCase( "amazon.titan" ) ) {
			return "titan";
		} else if ( id.findNoCase( "meta.llama" ) ) {
			return "llama";
		} else if ( id.findNoCase( "cohere" ) ) {
			return "cohere";
		} else if ( id.findNoCase( "ai21" ) ) {
			return "ai21";
		} else if ( id.findNoCase( "mistral" ) ) {
			return "mistral";
		} else if ( id.findNoCase( "openai" ) || id.findNoCase( "gpt" ) ) {
			// OpenAI-compatible models on Bedrock (cross-region inference profiles)
			return "openai";
		}

		return "unknown";
	}

	/**
	 * Transform an OpenAI-format request to Bedrock model-specific format
	 *
	 * @messages    Array of messages in OpenAI format
	 * @params      Additional parameters
	 * @modelFamily The detected model family
	 *
	 * @return Struct formatted for the specific Bedrock model
	 */
	private struct function transformRequestForModel(
		required array messages,
		required struct params,
		required string modelFamily
	){
		switch ( arguments.modelFamily ) {
			case "claude":
				return transformRequestForClaude( arguments.messages, arguments.params );
			case "titan":
				return transformRequestForTitan( arguments.messages, arguments.params );
			case "llama":
				return transformRequestForLlama( arguments.messages, arguments.params );
			case "mistral":
				return transformRequestForMistral( arguments.messages, arguments.params );
			case "openai":
				return transformRequestForOpenAI( arguments.messages, arguments.params );
			default:
				// Default to Claude-style format as it's most common on Bedrock
				return transformRequestForClaude( arguments.messages, arguments.params );
		}
	}

	/**
	 * Transform request for Claude models on Bedrock
	 * Uses Claude Messages API format with Bedrock-specific wrapper
	 */
	private struct function transformRequestForClaude( required array messages, required struct params ){
		var result = {
			"anthropic_version" : "bedrock-2023-05-31",
			"max_tokens"        : arguments.params.max_tokens ?: 4096
		};

		// Handle temperature if provided
		if ( arguments.params.keyExists( "temperature" ) ) {
			result[ "temperature" ] = arguments.params.temperature;
		}

		// Separate system message from other messages
		var systemPrompt         = "";
		var conversationMessages = [];

		for ( var msg in arguments.messages ) {
			if ( msg.role == "system" ) {
				systemPrompt = isSimpleValue( msg.content ) ? msg.content : msg.content.toString();
			} else {
				conversationMessages.append( {
					"role"    : msg.role,
					"content" : isSimpleValue( msg.content ) ? msg.content : msg.content.toString()
				} );
			}
		}

		result[ "messages" ] = conversationMessages;

		if ( len( trim( systemPrompt ) ) ) {
			result[ "system" ] = systemPrompt;
		}

		return result;
	}

	/**
	 * Transform request for Amazon Titan models
	 */
	private struct function transformRequestForTitan( required array messages, required struct params ){
		// Titan uses a simpler format with inputText
		var inputText = "";

		for ( var msg in arguments.messages ) {
			if ( msg.role == "system" ) {
				inputText &= "System: " & msg.content & char( 10 );
			} else if ( msg.role == "user" ) {
				inputText &= "User: " & msg.content & char( 10 );
			} else if ( msg.role == "assistant" ) {
				inputText &= "Assistant: " & msg.content & char( 10 );
			}
		}

		inputText &= "Assistant: ";

		var result = {
			"inputText"            : inputText,
			"textGenerationConfig" : {
				"maxTokenCount" : arguments.params.max_tokens ?: 4096,
				"temperature"   : arguments.params.temperature ?: 0.7,
				"topP"          : arguments.params.top_p ?: 0.9
			}
		};

		return result;
	}

	/**
	 * Transform request for Meta Llama models
	 */
	private struct function transformRequestForLlama( required array messages, required struct params ){
		// Llama uses a prompt-based format
		var prompt = "";

		for ( var msg in arguments.messages ) {
			if ( msg.role == "system" ) {
				prompt &= "<<SYS>>" & char( 10 ) & msg.content & char( 10 ) & "<</SYS>>" & char( 10 ) & char( 10 );
			} else if ( msg.role == "user" ) {
				prompt &= "[INST] " & msg.content & " [/INST]" & char( 10 );
			} else if ( msg.role == "assistant" ) {
				prompt &= msg.content & char( 10 );
			}
		}

		return {
			"prompt"      : prompt,
			"max_gen_len" : arguments.params.max_tokens ?: 2048,
			"temperature" : arguments.params.temperature ?: 0.7,
			"top_p"       : arguments.params.top_p ?: 0.9
		};
	}

	/**
	 * Transform request for Mistral models
	 */
	private struct function transformRequestForMistral( required array messages, required struct params ){
		// Mistral uses a similar format to OpenAI
		var prompt = "";

		for ( var msg in arguments.messages ) {
			if ( msg.role == "user" ) {
				prompt &= "<s>[INST] " & msg.content & " [/INST]";
			} else if ( msg.role == "assistant" ) {
				prompt &= msg.content & "</s>";
			} else if ( msg.role == "system" ) {
				// Prepend system message to first user message
				prompt = msg.content & char( 10 ) & char( 10 ) & prompt;
			}
		}

		return {
			"prompt"      : prompt,
			"max_tokens"  : arguments.params.max_tokens ?: 4096,
			"temperature" : arguments.params.temperature ?: 0.7,
			"top_p"       : arguments.params.top_p ?: 0.9
		};
	}

	/**
	 * Transform request for OpenAI-compatible models on Bedrock (cross-region inference profiles)
	 * These models use the standard OpenAI API format
	 */
	private struct function transformRequestForOpenAI( required array messages, required struct params ){
		// OpenAI-compatible models use the standard chat completions format
		var result = {
			"messages" : arguments.messages,
			"max_tokens" : arguments.params.max_tokens ?: 4096
		};

		// Handle temperature if provided
		if ( arguments.params.keyExists( "temperature" ) ) {
			result[ "temperature" ] = arguments.params.temperature;
		}

		// Handle top_p if provided
		if ( arguments.params.keyExists( "top_p" ) ) {
			result[ "top_p" ] = arguments.params.top_p;
		}

		return result;
	}

	/**
	 * Transform Bedrock response to OpenAI-compatible format
	 *
	 * @response    The raw response from Bedrock
	 * @modelFamily The model family
	 * @modelId     The model ID
	 *
	 * @return OpenAI-compatible response struct
	 */
	private struct function transformResponseFromModel(
		required struct response,
		required string modelFamily,
		required string modelId
	){
		switch ( arguments.modelFamily ) {
			case "claude":
				return transformResponseFromClaude( arguments.response, arguments.modelId );
			case "titan":
				return transformResponseFromTitan( arguments.response, arguments.modelId );
			case "llama":
				return transformResponseFromLlama( arguments.response, arguments.modelId );
			case "mistral":
				return transformResponseFromMistral( arguments.response, arguments.modelId );
			case "openai":
				return transformResponseFromOpenAI( arguments.response, arguments.modelId );
			default:
				return transformResponseFromClaude( arguments.response, arguments.modelId );
		}
	}

	/**
	 * Transform Claude Bedrock response to OpenAI format
	 */
	private struct function transformResponseFromClaude( required struct response, required string modelId ){
		var content = "";

		if (
			arguments.response.keyExists( "content" ) && isArray( arguments.response.content ) && arguments.response.content.len() > 0
		) {
			var contentBlock = arguments.response.content[ 1 ];
			if ( isStruct( contentBlock ) && contentBlock.keyExists( "text" ) ) {
				content = contentBlock.text;
			}
		}

		return {
			"choices" : [
				{
					"message"       : { "role" : "assistant", "content" : content },
					"finish_reason" : arguments.response.stop_reason ?: "stop",
					"index"         : 0
				}
			],
			"usage" : {
				"prompt_tokens"     : arguments.response.usage?.input_tokens ?: 0,
				"completion_tokens" : arguments.response.usage?.output_tokens ?: 0,
				"total_tokens"      : ( arguments.response.usage?.input_tokens ?: 0 ) + (
					arguments.response.usage?.output_tokens ?: 0
				)
			},
			"model" : arguments.modelId
		};
	}

	/**
	 * Transform Titan response to OpenAI format
	 */
	private struct function transformResponseFromTitan( required struct response, required string modelId ){
		var content = "";

		if (
			arguments.response.keyExists( "results" ) && isArray( arguments.response.results ) && arguments.response.results.len() > 0
		) {
			content = arguments.response.results[ 1 ].outputText ?: "";
		}

		return {
			"choices" : [
				{
					"message"       : { "role" : "assistant", "content" : content },
					"finish_reason" : "stop",
					"index"         : 0
				}
			],
			"usage" : {
				"prompt_tokens"     : arguments.response.inputTextTokenCount ?: 0,
				"completion_tokens" : arguments.response.results[ 1 ]?.tokenCount ?: 0,
				"total_tokens"      : ( arguments.response.inputTextTokenCount ?: 0 ) + (
					arguments.response.results[ 1 ]?.tokenCount ?: 0
				)
			},
			"model" : arguments.modelId
		};
	}

	/**
	 * Transform Llama response to OpenAI format
	 */
	private struct function transformResponseFromLlama( required struct response, required string modelId ){
		return {
			"choices" : [
				{
					"message" : {
						"role"    : "assistant",
						"content" : arguments.response.generation ?: ""
					},
					"finish_reason" : arguments.response.stop_reason ?: "stop",
					"index"         : 0
				}
			],
			"usage" : {
				"prompt_tokens"     : arguments.response.prompt_token_count ?: 0,
				"completion_tokens" : arguments.response.generation_token_count ?: 0,
				"total_tokens"      : ( arguments.response.prompt_token_count ?: 0 ) + (
					arguments.response.generation_token_count ?: 0
				)
			},
			"model" : arguments.modelId
		};
	}

	/**
	 * Transform Mistral response to OpenAI format
	 */
	private struct function transformResponseFromMistral( required struct response, required string modelId ){
		var content = "";

		if (
			arguments.response.keyExists( "outputs" ) && isArray( arguments.response.outputs ) && arguments.response.outputs.len() > 0
		) {
			content = arguments.response.outputs[ 1 ].text ?: "";
		}

		return {
			"choices" : [
				{
					"message"       : { "role" : "assistant", "content" : content },
					"finish_reason" : arguments.response.outputs[ 1 ]?.stop_reason ?: "stop",
					"index"         : 0
				}
			],
			"usage" : {
				"prompt_tokens"     : 0,
				"completion_tokens" : 0,
				"total_tokens"      : 0
			},
			"model" : arguments.modelId
		};
	}

	/**
	 * Transform OpenAI-compatible response from Bedrock
	 * These models already return OpenAI-format responses, so minimal transformation needed
	 */
	private struct function transformResponseFromOpenAI( required struct response, required string modelId ){
		// OpenAI-compatible models already return the standard format
		// Just ensure all expected fields exist
		var content = "";

		// Try standard OpenAI response format first
		if (
			arguments.response.keyExists( "choices" ) && isArray( arguments.response.choices ) && arguments.response.choices.len() > 0
		) {
			// Already in OpenAI format, just return it with model ID override
			var result = duplicate( arguments.response );
			result[ "model" ] = arguments.modelId;
			return result;
		}

		// Fallback: check for message content directly (some variations)
		if ( arguments.response.keyExists( "message" ) && arguments.response.message.keyExists( "content" ) ) {
			content = arguments.response.message.content;
		} else if ( arguments.response.keyExists( "content" ) ) {
			// Direct content field
			if ( isArray( arguments.response.content ) && arguments.response.content.len() > 0 ) {
				var contentBlock = arguments.response.content[ 1 ];
				if ( isStruct( contentBlock ) && contentBlock.keyExists( "text" ) ) {
					content = contentBlock.text;
				}
			} else if ( isSimpleValue( arguments.response.content ) ) {
				content = arguments.response.content;
			}
		}

		return {
			"choices" : [
				{
					"message"       : { "role" : "assistant", "content" : content },
					"finish_reason" : arguments.response.finish_reason ?: arguments.response.stop_reason ?: "stop",
					"index"         : 0
				}
			],
			"usage" : {
				"prompt_tokens"     : arguments.response.usage?.prompt_tokens ?: arguments.response.usage?.input_tokens ?: 0,
				"completion_tokens" : arguments.response.usage?.completion_tokens ?: arguments.response.usage?.output_tokens ?: 0,
				"total_tokens"      : arguments.response.usage?.total_tokens ?: 0
			},
			"model" : arguments.modelId
		};
	}

	/**
	 * Override chat to handle Bedrock-specific request signing and model transformation
	 *
	 * @chatRequest      The AiChatRequest object
	 * @interactionCount Current tool call interaction count
	 */
		@override
	public function chat( required AiChatRequest chatRequest, numeric interactionCount = 0 ){
		// Get the model to use
		var modelId = len( variables.modelId ) ? variables.modelId : arguments.chatRequest.getModel();
		if ( !len( modelId ) ) {
			modelId = variables.params.model;
		}

		// Determine the endpoint identifier (use inference profile ARN if provided)
		var inferenceProfileArn = arguments.chatRequest.getProviderOption( "inferenceProfileArn" );
		var endpointId = len( inferenceProfileArn ) ? inferenceProfileArn : modelId;

		// Detect model family for request/response transformation (always use modelId for this)
		var modelFamily = detectModelFamily( modelId );

		// Build the request body for the specific model
		var dataPacket = transformRequestForModel(
			messages    = arguments.chatRequest.getMessages(),
			params      = arguments.chatRequest.getParams(),
			modelFamily = modelFamily
		);

		// Send the request to Bedrock (use endpointId which may be an inference profile ARN)
		var result = sendBedrockRequest( endpointId, dataPacket, arguments.chatRequest );

		// Check for errors
		if ( result.keyExists( "error" ) ) {
			writeLog(
				text: "Bedrock error: #result.error.toString()#",
				type: "error",
				log : "ai"
			);

			BoxAnnounce(
				"onAIError",
				{
					error        : result.error,
					errorMessage : result.error.toString(),
					provider     : this,
					operation    : "chat",
					chatRequest  : arguments.chatRequest,
					canRetry     : true
				}
			);

			throw( type: "ProviderError", message: result.error.toString() );
		}

		// Transform response to OpenAI format
		var openAIResponse = transformResponseFromModel( result, modelFamily, modelId );

		// Announce token usage
		if ( openAIResponse.keyExists( "usage" ) ) {
			BoxAnnounce(
				"onAITokenCount",
				{
					provider            : this,
					operation           : "chat",
					model               : modelId,
					promptTokens        : openAIResponse.usage.prompt_tokens,
					completionTokens    : openAIResponse.usage.completion_tokens,
					totalTokens         : openAIResponse.usage.total_tokens,
					chatRequest         : arguments.chatRequest,
					usage               : openAIResponse.usage,
					// Multi-tenant tracking fields
					tenantId            : arguments.chatRequest.getTenantId(),
					usageMetadata       : arguments.chatRequest.getUsageMetadata(),
					inferenceProfileArn : arguments.chatRequest.getProviderOption( "inferenceProfileArn" ),
					timestamp           : now()
				}
			);
		}

		// Handle response based on return format
		var content = openAIResponse.choices[ 1 ].message.content;

		switch ( arguments.chatRequest.getReturnFormat() ) {
			case "all":
				return openAIResponse.choices;
			case "raw":
				return openAIResponse;
			case "json":
				return jsonDeserialize( extractFromCodeBlock( content, "json" ) );
			case "xml":
				return xmlParse( extractFromCodeBlock( content, "xml" ) );
			case "structuredOutput":
				return populateStructuredOutput( content, arguments.chatRequest );
			case "single":
			default:
				return content;
		}
	}

	/**
	 * Send a signed request to AWS Bedrock
	 *
	 * @modelId    The model ID or inference profile ARN to invoke
	 * @dataPacket The request payload
	 * @aiRequest  The AiChatRequest for logging settings
	 *
	 * @return The parsed response from Bedrock
	 */
	private struct function sendBedrockRequest(
		required string modelId,
		required struct dataPacket,
		required AiChatRequest aiRequest
	){
		var host     = getBedrockHost();
		var path     = getBedrockPath( arguments.modelId );
		var endpoint = getBedrockEndpoint( arguments.modelId );
		var payload  = jsonSerialize( arguments.dataPacket );

		// Announce the request (aligned with BaseService event contract)
		BoxAnnounce(
			"onAIChatRequest",
			{
				"dataPacket"    : arguments.dataPacket,
				"chatRequest"   : arguments.aiRequest,
				"provider"      : this,
				"auditMetadata" : arguments.aiRequest.getAuditMetadata()
			}
		);

		// Log the request if enabled
		if ( arguments.aiRequest.getLogRequest() ) {
			writeLog(
				text: "Bedrock Request to #endpoint#: #payload#",
				type: "info",
				log : "ai"
			);
		}

		if ( arguments.aiRequest.getLogRequestToConsole() ) {
			println( "Bedrock Request" );
			println( arguments.dataPacket );
		}

		// Sign the request using AWS Signature V4
		var signer        = new AwsSignatureV4();
		var signedHeaders = signer.signRequest(
			method      = "POST",
			host        = host,
			path        = path,
			queryString = "",
			headers     = {
				"content-type" : "application/json",
				"accept"       : "application/json"
			},
			payload         = payload,
			region          = variables.region,
			service         = static.AWS_SERVICE,
			accessKeyId     = variables.awsAccessKeyId,
			secretAccessKey = variables.awsSecretAccessKey,
			sessionToken    = variables.awsSessionToken
		);

		// Make the HTTP request using fluent API
		var httpService = http( url = endpoint )
			.method( "POST" )
			.charset( "UTF-8" )
			.timeout( arguments.aiRequest.getTimeout() )
			.throwOnError( false )
			.header( "Content-Type", "application/json" )
			.header( "Accept", "application/json" )
			.header( "Host", host );

		// Add signed headers
		for ( var headerName in signedHeaders ) {
			httpService.header( headerName, signedHeaders[ headerName ] );
		}

		// Add body and send
		var httpResult = httpService.body( payload ).send();

		// Log response if enabled
		if ( arguments.aiRequest.getLogResponse() ) {
			writeLog(
				text: "Bedrock Response: #httpResult.fileContent#",
				type: "info",
				log : "ai"
			);
		}

		if ( arguments.aiRequest.getLogResponseToConsole() ) {
			println( "Bedrock Response" );
			println( httpResult.fileContent );
		}

		// Check for HTTP errors
		var statusCode = val( listFirst( httpResult.statusCode, " " ) );
		if ( statusCode != 200 ) {
			var errorResponse = {
				"error" : {
					"message"    : "Bedrock request failed with status #statusCode#: #httpResult.fileContent#",
					"statusCode" : statusCode,
					"response"   : httpResult.fileContent
				}
			};

			BoxAnnounce(
				"onAIError",
				{
					error        : errorResponse.error,
					errorMessage : errorResponse.error.message,
					provider     : this,
					operation    : "chat",
					chatRequest  : arguments.aiRequest,
					canRetry     : statusCode >= 500
				}
			);

			return errorResponse;
		}

		// Parse and return response
		var response = jsonDeserialize( httpResult.fileContent );

		BoxAnnounce(
			"onAIChatResponse",
			{
				"chatRequest" : arguments.aiRequest,
				"response"    : response,
				"rawResponse" : httpResult,
				"provider"    : this
			}
		);

		return response;
	}

	/**
	 * Stream chat responses from Bedrock using InvokeModelWithResponseStream
	 *
	 * Bedrock streaming uses AWS event-stream format which is different from SSE.
	 * Each event contains headers and a payload with the streaming chunk.
	 *
	 * @chatRequest The AiChatRequest object
	 * @callback    A callback function to be called with each chunk: function( chunk )
	 */
	@override
	public function chatStream( required AiChatRequest chatRequest, required function callback ){
		// Get the model to use
		var modelId = len( variables.modelId ) ? variables.modelId : arguments.chatRequest.getModel();
		if ( !len( modelId ) ) {
			modelId = variables.params.model;
		}

		// Determine the endpoint identifier (use inference profile ARN if provided)
		var inferenceProfileArn = arguments.chatRequest.getProviderOption( "inferenceProfileArn" );
		var endpointId = len( inferenceProfileArn ) ? inferenceProfileArn : modelId;

		// Detect model family for request/response transformation (always use modelId for this)
		var modelFamily = detectModelFamily( modelId );

		// Build the request body for the specific model
		var dataPacket = transformRequestForModel(
			messages    = arguments.chatRequest.getMessages(),
			params      = arguments.chatRequest.getParams(),
			modelFamily = modelFamily
		);

		// Send the streaming request (use endpointId which may be an inference profile ARN)
		sendBedrockStreamRequest(
			modelId     = endpointId,
			dataPacket  = dataPacket,
			aiRequest   = arguments.chatRequest,
			modelFamily = modelFamily,
			callback    = arguments.callback
		);
	}

	/**
	 * Send a signed streaming request to AWS Bedrock
	 *
	 * AWS Bedrock streaming uses binary event-stream format (application/vnd.amazon.eventstream)
	 * which is different from SSE. Since BoxLang's HTTP streaming is designed for SSE,
	 * we make a synchronous request and parse the binary event-stream response.
	 *
	 * @modelId     The model ID or inference profile ARN to invoke
	 * @dataPacket  The request payload
	 * @aiRequest   The AiChatRequest for logging settings
	 * @modelFamily The detected model family for response transformation
	 * @callback    The callback function for streaming chunks
	 */
	private function sendBedrockStreamRequest(
		required string modelId,
		required struct dataPacket,
		required AiChatRequest aiRequest,
		required string modelFamily,
		required function callback
	){
		var host     = getBedrockHost();
		var path     = getBedrockStreamPath( arguments.modelId );
		var endpoint = getBedrockStreamEndpoint( arguments.modelId );
		var payload  = jsonSerialize( arguments.dataPacket );

		// Announce the request (aligned with BaseService event contract)
		BoxAnnounce(
			"onAIChatRequest",
			{
				"dataPacket"    : arguments.dataPacket,
				"chatRequest"   : arguments.aiRequest,
				"provider"      : this,
				"auditMetadata" : arguments.aiRequest.getAuditMetadata()
			}
		);

		// Log the request if enabled
		if ( arguments.aiRequest.getLogRequest() ) {
			writeLog(
				text: "Bedrock Stream Request to #endpoint#: #payload#",
				type: "info",
				log : "ai"
			);
			// Debug: Log credential status
			writeLog(
				text: "Bedrock Stream Credentials - Region: #variables.region#, HasAccessKey: #len( variables.awsAccessKeyId ) > 0#, HasSecretKey: #len( variables.awsSecretAccessKey ) > 0#, HasSessionToken: #len( variables.awsSessionToken ) > 0#, SessionTokenLen: #len( variables.awsSessionToken )#",
				type: "info",
				log : "ai"
			);
		}

		if ( arguments.aiRequest.getLogRequestToConsole() ) {
			println( "Bedrock Stream Request" );
			println( arguments.dataPacket );
		}

		// Sign the request using AWS Signature V4
		var signer        = new AwsSignatureV4();
		var signedHeaders = signer.signRequest(
			method      = "POST",
			host        = host,
			path        = path,
			queryString = "",
			headers     = {
				"content-type" : "application/json",
				"accept"       : "application/vnd.amazon.eventstream"
			},
			payload         = payload,
			region          = variables.region,
			service         = static.AWS_SERVICE,
			accessKeyId     = variables.awsAccessKeyId,
			secretAccessKey = variables.awsSecretAccessKey,
			sessionToken    = variables.awsSessionToken
		);

		// Build HTTP request - use synchronous request and process binary response
		var httpService = http( url = endpoint )
			.method( "POST" )
			.charset( "UTF-8" )
			.timeout( arguments.aiRequest.getTimeout() )
			.throwOnError( false )
			.header( "Content-Type", "application/json" )
			.header( "Accept", "application/vnd.amazon.eventstream" )
			.header( "Host", host );

		// Add signed headers
		for ( var headerName in signedHeaders ) {
			httpService.header( headerName, signedHeaders[ headerName ] );
		}

		// Send request and get response
		var httpResult = httpService.body( payload ).send();

		// Check for HTTP errors
		var statusCode = val( listFirst( httpResult.statusCode, " " ) );
		if ( statusCode != 200 ) {
			var errorMessage = "Bedrock stream request failed with status #statusCode#: #httpResult.fileContent#";
			writeLog( text: errorMessage, type: "error", log: "ai" );

			BoxAnnounce(
				"onAIError",
				{
					error        : { "message" : errorMessage, "statusCode" : statusCode },
					errorMessage : errorMessage,
					provider     : this,
					operation    : "stream",
					chatRequest  : arguments.aiRequest,
					canRetry     : statusCode >= 500
				}
			);

			throw( type: "ProviderError", message: errorMessage );
		}

		// Parse the AWS event-stream binary response
		// The response contains multiple events, each with a binary header and JSON payload
		parseBedrockEventStream(
			httpResult.fileContent,
			arguments.modelFamily,
			arguments.modelId,
			arguments.callback
		);

		// Log completion
		if ( arguments.aiRequest.getLogResponse() ) {
			writeLog(
				text: "Bedrock stream completed successfully",
				type: "info",
				log : "ai"
			);
		}

		BoxAnnounce(
			"onAIChatResponse",
			{
				"chatRequest"    : arguments.aiRequest,
				"provider"       : this,
				"streamComplete" : true
			}
		);
	}

	/**
	 * Parse AWS event-stream binary format from Bedrock response
	 *
	 * AWS event-stream format:
	 * - Each event has: prelude (8 bytes) + headers + payload + CRC (4 bytes)
	 * - Prelude: total_length (4 bytes) + headers_length (4 bytes)
	 * - The payload is typically JSON with a "bytes" field containing base64-encoded data
	 *
	 * @responseData  The raw response data from Bedrock
	 * @modelFamily   The model family for response transformation
	 * @modelId       The model ID
	 * @callback      The callback function for streaming chunks
	 */
	private function parseBedrockEventStream(
		required any responseData,
		required string modelFamily,
		required string modelId,
		required function callback
	){
		var data = arguments.responseData;

		// Debug: Log raw response info
		writeLog(
			text: "parseBedrockEventStream - Raw data type: #( isBinary( data ) ? 'binary' : 'string' )#, length: #( isBinary( data ) ? arrayLen( data ) : len( data ) )#",
			type: "info",
			log : "ai"
		);

		// AWS event-stream is a binary protocol. We need to parse it properly.
		// Format: Each event consists of:
		// - Prelude (8 bytes): total_length (4 bytes, big-endian) + headers_length (4 bytes, big-endian)
		// - Prelude CRC (4 bytes)
		// - Headers (variable length)
		// - Payload (variable length) - contains JSON with "bytes" field holding base64 data
		// - Message CRC (4 bytes)

		if ( !isBinary( data ) ) {
			// If somehow we got a string, try to find JSON in it
			var jsonMatches = reMatchNoCase( '\{"bytes":"[^"]+"\}', data );
			for ( var match in jsonMatches ) {
				processBedrockEvent( match, arguments.modelFamily, arguments.modelId, arguments.callback );
			}
			return;
		}

		// Parse binary event-stream
		var bytes      = data;
		var dataLength = arrayLen( bytes );
		var position   = 1; // BoxLang arrays are 1-indexed
		var eventCount = 0;

		writeLog(
			text: "parseBedrockEventStream - Parsing binary stream, total bytes: #dataLength#",
			type: "info",
			log : "ai"
		);

		while ( position + 12 <= dataLength ) {
			// Read prelude: total_length (4 bytes) + headers_length (4 bytes)
			var totalLength = readBigEndianInt( bytes, position );
			var headersLength = readBigEndianInt( bytes, position + 4 );

			// Sanity check
			if ( totalLength < 16 || totalLength > 1000000 || headersLength < 0 || headersLength > totalLength ) {
				writeLog(
					text: "parseBedrockEventStream - Invalid event at position #position#: totalLength=#totalLength#, headersLength=#headersLength#",
					type: "warning",
					log : "ai"
				);
				break;
			}

			// Calculate payload position and length
			// Event structure: prelude(8) + preludeCRC(4) + headers(headersLength) + payload + messageCRC(4)
			var payloadStart  = position + 8 + 4 + headersLength; // After prelude + preludeCRC + headers
			var payloadLength = totalLength - 8 - 4 - headersLength - 4; // Subtract prelude, preludeCRC, headers, messageCRC

			if ( payloadLength > 0 && payloadStart + payloadLength - 1 <= dataLength ) {
				// Extract payload bytes
				var payloadBytes = [];
				for ( var i = payloadStart; i < payloadStart + payloadLength; i++ ) {
					arrayAppend( payloadBytes, bytes[ i ] );
				}

				// Convert payload to string and parse
				try {
					var payloadStr = toString( javaCast( "byte[]", payloadBytes ), "UTF-8" );

					// Debug first event payload
					if ( eventCount == 0 ) {
						writeLog(
							text: "parseBedrockEventStream - First event payload (#payloadLength# bytes): #left( payloadStr, 300 )#",
							type: "info",
							log : "ai"
						);
					}

					processBedrockEvent( payloadStr, arguments.modelFamily, arguments.modelId, arguments.callback );
					eventCount++;
				} catch ( any e ) {
					writeLog(
						text: "parseBedrockEventStream - Error processing event at position #position#: #e.message#",
						type: "warning",
						log : "ai"
					);
				}
			}

			// Move to next event
			position += totalLength;
		}

		writeLog(
			text: "parseBedrockEventStream - Processed #eventCount# events",
			type: "info",
			log : "ai"
		);
	}

	/**
	 * Read a 4-byte big-endian integer from a byte array
	 */
	private numeric function readBigEndianInt( required array bytes, required numeric position ){
		// Convert signed bytes to unsigned and combine
		var b1 = arguments.bytes[ arguments.position ] < 0 ? arguments.bytes[ arguments.position ] + 256 : arguments.bytes[ arguments.position ];
		var b2 = arguments.bytes[ arguments.position + 1 ] < 0 ? arguments.bytes[ arguments.position + 1 ] + 256 : arguments.bytes[ arguments.position + 1 ];
		var b3 = arguments.bytes[ arguments.position + 2 ] < 0 ? arguments.bytes[ arguments.position + 2 ] + 256 : arguments.bytes[ arguments.position + 2 ];
		var b4 = arguments.bytes[ arguments.position + 3 ] < 0 ? arguments.bytes[ arguments.position + 3 ] + 256 : arguments.bytes[ arguments.position + 3 ];

		return ( b1 * 16777216 ) + ( b2 * 65536 ) + ( b3 * 256 ) + b4;
	}

	/**
	 * Process a single Bedrock event payload
	 */
	private function processBedrockEvent(
		required string payloadStr,
		required string modelFamily,
		required string modelId,
		required function callback
	){
		// The payload should be JSON. It may contain a "bytes" field with base64 data
		// or be direct JSON from the model.
		try {
			var payloadJson = jsonDeserialize( arguments.payloadStr );

			// Check if this is a wrapper with base64 "bytes" field
			if ( payloadJson.keyExists( "bytes" ) ) {
				var decodedPayload = toString( binaryDecode( payloadJson.bytes, "base64" ), "UTF-8" );
				payloadJson = jsonDeserialize( decodedPayload );
			}

			// Transform and send to callback
			var streamChunk = transformStreamChunk( payloadJson, arguments.modelFamily, arguments.modelId );
			if ( !isNull( streamChunk ) ) {
				arguments.callback( streamChunk );
			}
		} catch ( any e ) {
			// Not valid JSON or processing error - may be a control message
			writeLog(
				text: "processBedrockEvent - Could not process: #left( arguments.payloadStr, 100 )# - Error: #e.message#",
				type: "debug",
				log : "ai"
			);
		}
	}

	/**
	 * Transform a streaming chunk from Bedrock model-specific format to OpenAI-compatible format
	 *
	 * @chunk       The raw chunk from Bedrock
	 * @modelFamily The model family
	 * @modelId     The model ID
	 *
	 * @return OpenAI-compatible streaming chunk or null if no content
	 */
	private function transformStreamChunk(
		required struct chunk,
		required string modelFamily,
		required string modelId
	){
		var content = "";
		var finishReason = "";

		switch ( arguments.modelFamily ) {
			case "claude":
				// Claude streaming format
				if ( arguments.chunk.keyExists( "type" ) ) {
					switch ( arguments.chunk.type ) {
						case "content_block_delta":
							if ( arguments.chunk.keyExists( "delta" ) && arguments.chunk.delta.keyExists( "text" ) ) {
								content = arguments.chunk.delta.text;
							}
							break;
						case "message_stop":
							finishReason = "stop";
							break;
						case "message_delta":
							if ( arguments.chunk.keyExists( "delta" ) && arguments.chunk.delta.keyExists( "stop_reason" ) ) {
								finishReason = arguments.chunk.delta.stop_reason ?: "stop";
							}
							break;
						default:
							// Skip other event types like message_start, content_block_start
							return javacast( "null", "" );
					}
				}
				break;

			case "titan":
				// Titan streaming format
				if ( arguments.chunk.keyExists( "outputText" ) ) {
					content = arguments.chunk.outputText;
				}
				if ( arguments.chunk.keyExists( "completionReason" ) && len( arguments.chunk.completionReason ) ) {
					finishReason = arguments.chunk.completionReason == "FINISH" ? "stop" : arguments.chunk.completionReason.lcase();
				}
				break;

			case "llama":
				// Llama streaming format
				if ( arguments.chunk.keyExists( "generation" ) ) {
					content = arguments.chunk.generation;
				}
				if ( arguments.chunk.keyExists( "stop_reason" ) && len( arguments.chunk.stop_reason ?: "" ) ) {
					finishReason = arguments.chunk.stop_reason;
				}
				break;

			case "mistral":
				// Mistral streaming format
				if ( arguments.chunk.keyExists( "outputs" ) && isArray( arguments.chunk.outputs ) && arguments.chunk.outputs.len() > 0 ) {
					content = arguments.chunk.outputs[ 1 ].text ?: "";
					if ( arguments.chunk.outputs[ 1 ].keyExists( "stop_reason" ) ) {
						finishReason = arguments.chunk.outputs[ 1 ].stop_reason ?: "";
					}
				}
				break;

			case "openai":
				// OpenAI-compatible streaming format - already in standard format
				if ( arguments.chunk.keyExists( "choices" ) && isArray( arguments.chunk.choices ) && arguments.chunk.choices.len() > 0 ) {
					var delta = arguments.chunk.choices[ 1 ].delta ?: {};
					if ( delta.keyExists( "content" ) ) {
						content = delta.content;
					}
					if ( arguments.chunk.choices[ 1 ].keyExists( "finish_reason" ) && !isNull( arguments.chunk.choices[ 1 ].finish_reason ) ) {
						finishReason = arguments.chunk.choices[ 1 ].finish_reason;
					}
				}
				break;

			default:
				// Default to Claude format
				if ( arguments.chunk.keyExists( "delta" ) && arguments.chunk.delta.keyExists( "text" ) ) {
					content = arguments.chunk.delta.text;
				}
		}

		// Only return if we have content or a finish reason
		if ( !len( content ) && !len( finishReason ) ) {
			return javacast( "null", "" );
		}

		// Return OpenAI-compatible streaming format
		return {
			"choices" : [
				{
					"delta" : len( content ) ? { "content" : content } : {},
					"index" : 0,
					"finish_reason" : len( finishReason ) ? finishReason : javacast( "null", "" )
				}
			],
			"model" : arguments.modelId
		};
	}

	/**
	 * Generate embeddings using Bedrock Titan Embeddings or other embedding models
	 *
	 * @embeddingRequest The embedding request object
	 */
		@override
	public function embeddings( required AiEmbeddingRequest embeddingRequest ){
		// Get the embedding model - default to Titan Embeddings
		var modelId = arguments.embeddingRequest.getModel();
		if ( !len( modelId ) ) {
			modelId = "amazon.titan-embed-text-v1";
		}

		var input   = arguments.embeddingRequest.getInput();
		var results = [];

		// Titan embeddings handles one text at a time
		var textsToProcess = isArray( input ) ? input : [ input ];

		for ( var i = 1; i <= textsToProcess.len(); i++ ) {
			var text = textsToProcess[ i ];

			var dataPacket = { "inputText" : text };

			var result = sendBedrockEmbeddingRequest(
				modelId,
				dataPacket,
				arguments.embeddingRequest
			);

			if ( result.keyExists( "error" ) ) {
				throw( type: "ProviderError", message: result.error.message ?: "Embedding request failed" );
			}

			arrayAppend( results, { "embedding" : result.embedding, "index" : i - 1 } );
		}

		// Format response like OpenAI
		var response = {
			"data"  : results,
			"model" : modelId,
			"usage" : { "prompt_tokens" : 0, "total_tokens" : 0 }
		};

		// Return based on format
		switch ( arguments.embeddingRequest.getReturnFormat() ) {
			case "embeddings":
				return response.data.map( item => item.embedding );
			case "raw":
				return response;
			case "first":
			default:
				return response.data[ 1 ].embedding;
		}
	}

	/**
	 * Send a signed embedding request to Bedrock
	 */
	private struct function sendBedrockEmbeddingRequest(
		required string modelId,
		required struct dataPacket,
		required AiEmbeddingRequest embeddingRequest
	){
		var host     = getBedrockHost();
		var path     = getBedrockPath( arguments.modelId );
		var endpoint = getBedrockEndpoint( arguments.modelId );
		var payload  = jsonSerialize( arguments.dataPacket );

		// Sign the request
		var signer        = new AwsSignatureV4();
		var signedHeaders = signer.signRequest(
			method      = "POST",
			host        = host,
			path        = path,
			queryString = "",
			headers     = {
				"content-type" : "application/json",
				"accept"       : "application/json"
			},
			payload         = payload,
			region          = variables.region,
			service         = static.AWS_SERVICE,
			accessKeyId     = variables.awsAccessKeyId,
			secretAccessKey = variables.awsSecretAccessKey,
			sessionToken    = variables.awsSessionToken
		);

		// Make the request using fluent API
		var httpService = http( url = endpoint )
			.method( "POST" )
			.charset( "UTF-8" )
			.timeout( arguments.embeddingRequest.getTimeout() )
			.throwOnError( false )
			.header( "Content-Type", "application/json" )
			.header( "Accept", "application/json" )
			.header( "Host", host );

		// Add signed headers
		for ( var headerName in signedHeaders ) {
			httpService.header( headerName, signedHeaders[ headerName ] );
		}

		// Add body and send
		var httpResult = httpService.body( payload ).send();

		var statusCode = val( listFirst( httpResult.statusCode, " " ) );
		if ( statusCode != 200 ) {
			return {
				"error" : {
					"message"    : "Bedrock embedding request failed: #httpResult.fileContent#",
					"statusCode" : statusCode
				}
			};
		}

		return jsonDeserialize( httpResult.fileContent );
	}
}
