/**
 * [BoxLang]
 *
 * Copyright [2023] [Ortus Solutions, Corp]
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the
 * License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS"
 * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language
 * governing permissions and limitations under the License.
 * ----------------------------------------------------------------------------------
 * Docker Model Runner Service
 *
 * This service provides integration with Docker Model Runner, which exposes
 * OpenAI-compatible APIs for running local AI models.
 *
 * Docker Model Runner allows running models locally with GPU acceleration
 * through Docker Desktop's AI features.
 *
 * This service extends OpenAICompatibleService with Docker-specific defaults
 * and configuration options.
 *
 * @see https://docs.docker.com/ai/model-runner/
 */
class extends = "OpenAICompatibleService"{
	/**
	 * Static defaults
	 */
	static {
		// Default endpoint for Docker Model Runner (accessible from Docker containers)
		DEFAULT_BASE_URL = "http://model-runner.docker.internal/v1";
		// Alternative for host machine access
		HOST_BASE_URL    = "http://localhost:12434/v1";

		DEFAULT_CHAT_PARAMS = {};
		DEFAULT_EMBED_PARAMS = {};
	}

	/**
	 * Constructor
	 */
	function init(){
		// Call parent constructor first
		super.init();

		// Override with Docker Model Runner specific defaults
		variables.baseURL       = static.DEFAULT_BASE_URL;
		variables.chatURL       = variables.baseURL & "/chat/completions";
		variables.embeddingsURL = variables.baseURL & "/embeddings";
		variables.name          = "DockerModelRunner";

		defaults( static.DEFAULT_CHAT_PARAMS );
	}

	/**
	 * Configure the service with options
	 *
	 *   - String: API key (optional, empty string for no auth)
	 *   - Struct: Configuration options (see OpenAICompatibleService.configure) plus:
	 *     - useHostURL: Boolean to use localhost URL instead of Docker internal
	 *
	 * @config Can be:
	 *
	 * @return The service instance for chaining
	 */
		@override
	IAiService function configure( required any config ){
		// Handle Docker-specific useHostURL option before calling parent
		if (
			isStruct( arguments.config ) && arguments.config.keyExists( "useHostURL" ) && arguments.config.useHostURL
		) {
			arguments.config.baseURL = static.HOST_BASE_URL;
		}

		// Delegate to parent for standard configuration
		return super.configure( arguments.config );
	}

	/**
	 * Override chat to handle Docker Model Runner specifics
	 * Adds automatic retry for model loading (503) errors.
	 *
	 * @chatRequest      The AiChatRequest object to send to the provider
	 * @interactionCount Current tool call interaction count (used internally)
	 * @retryCount       Current retry attempt for model loading (used internally)
	 *
	 * @return The response from the provider according to the return format
	 */
		@override
	public function chat(
		required AiChatRequest chatRequest,
		numeric interactionCount = 0,
		numeric retryCount       = 0
	){
		// Use the parent implementation with retry for model loading
		try {
			return super.chat( chatRequest = arguments.chatRequest, interactionCount = arguments.interactionCount );
		} catch ( ProviderError e ) {
			// Check if this is a "model loading" error (503)
			if ( e.message.findNoCase( "Loading model" ) > 0 || e.message.findNoCase( "503" ) > 0 ) {
				// Retry up to 10 times with longer delays for model loading
				// Large models can take 30-60+ seconds to load
				var maxRetries = 10;
				if ( arguments.retryCount < maxRetries ) {
					// Use 5 second intervals for consistent, longer waits during model load
					var waitTime = 5000; // 5 seconds between retries
					writeLog(
						text: "Docker Model Runner: Model loading, retrying in #waitTime / 1000#s (attempt #arguments.retryCount + 1#/#maxRetries#)",
						type: "info",
						log : "ai"
					);
					sleep( waitTime );
					return chat(
						arguments.chatRequest,
						arguments.interactionCount,
						arguments.retryCount + 1
					);
				}
			}
			// Re-throw if not a loading error or max retries exceeded
			rethrow;
		}
	}

	/**
	 * Override chatStream to handle Docker Model Runner specifics
	 * Adds automatic retry for model loading (503) errors.
	 *
	 * @chatRequest The AiChatRequest object to send to the provider
	 * @callback    A callback function to be called with each chunk of the stream
	 * @retryCount  Current retry attempt for model loading (used internally)
	 */
		@override
	public function chatStream(
		required AiChatRequest chatRequest,
		required function callback,
		numeric retryCount = 0
	){
		// Use the parent implementation with retry for model loading
		try {
			return super.chatStream( chatRequest = arguments.chatRequest, callback = arguments.callback );
		} catch ( ProviderError e ) {
			// Check if this is a "model loading" error (503)
			if ( e.message.findNoCase( "Loading model" ) > 0 || e.message.findNoCase( "503" ) > 0 ) {
				// Retry up to 10 times with longer delays for model loading
				// Large models can take 30-60+ seconds to load
				var maxRetries = 10;
				if ( arguments.retryCount < maxRetries ) {
					// Use 5 second intervals for consistent, longer waits during model load
					var waitTime = 5000; // 5 seconds between retries
					writeLog(
						text: "Docker Model Runner: Model loading for stream, retrying in #waitTime / 1000#s (attempt #arguments.retryCount + 1#/#maxRetries#)",
						type: "info",
						log : "ai"
					);
					sleep( waitTime );
					return chatStream(
						arguments.chatRequest,
						arguments.callback,
						arguments.retryCount + 1
					);
				}
			}
			// Re-throw if not a loading error or max retries exceeded
			rethrow;
		}
	}
}
