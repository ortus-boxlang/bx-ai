/**
 * [BoxLang]
 *
 * Copyright [2023] [Ortus Solutions, Corp]
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the
 * License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS"
 * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language
 * governing permissions and limitations under the License.
 * ----------------------------------------------------------------------------------
 * Base Service for AI Providers
 * This service provides a base implementation for AI providers
 * It is based on the Open AI standard, overridable by the provider
 *
 * All AI Providers must extend this service and implement the following methods:
 * - configure()
 * - invoke()
 * - invokeStream()
 *
 * All providers have the following properties:
 * - apiKey                                   : The API key to use with the provider
 * - chatURL                                  : The chat URL of the provider API
 * - params                                   : The default params request properties
 */
abstract class implements="IAiService" {

	/**
	 * The name of the LLM
	 */
	property name="name" default="";

	/**
	 * The API key to use with the provider
	 */
	property name = "apiKey" default = "";

	/**
	 * The chat URL of the provider API
	 */
	property name = "chatURL" default = "";

	/**
	 * The default params to use with the provider
	 */
	property name = "params" type = "struct" default = {};

	/**
	 * The default headers to use with the provider
	 */
	property name = "headers" type = "struct" default = {};

	/**
	 * Constants
	 */
	static {
		final settings = getModuleInfo( "bxai" ).settings
	}

	/**
	 * ---------------------------------------------------------------------------------------------------------
	 * Helper Methods
	 * ---------------------------------------------------------------------------------------------------------
	 */

	/**
	 * Set the default params for the provider
	 *
	 * @params - The params to set as defaults
	 *
	 * @return The service instance
	 */
	IAiService function defaults( required params ){
		variables.params.append( arguments.params, true );
		return this;
	}

	/**
	 * Add a header to the service definition
	 *
	 * @name The name of the header
	 * @value The value of the header
	 */
	BaseService function addHeader( required string name, required string value ){
		variables.headers[ arguments.name ] = arguments.value;
		return this;
	}

	/**
	 * ---------------------------------------------------------------------------------------------------------
	 * Interface Methods
	 * ---------------------------------------------------------------------------------------------------------
	 */

	 /**
	 * Configure the service with the API key
	 *
	 * @apiKey - The API key to use with the provider
	 *
	 * @return The service instance
	 */
	IAiService function configure( required any apiKey ){
		variables.apiKey = arguments.apiKey;
		return this;
	}

	/**
	 * Invoke a request to the provider
	 *
	 * @aiRequest The AI request to send to the provider
	 *
	 * @return The response from the provider according to the return format in the AI request
	 */
	function invoke( required AiRequest aiRequest ){
		// Model Selection if not set, use the default in the service, which should always be set
		aiRequest
			.setModelIfEmpty( variables.params.model )
			.setApiKeyIfEmpty( getAPIKey() )
			.mergeServiceParams( variables.params )
			.mergeServiceHeaders( variables.headers )
		// Do a chat request
		// MORE TYPES CAN BE ADDED HERE LATER
		return chat( argumentCollection = arguments )
	}

	/**
	 * Invoke a request to the provider in streaming mode
	 *
	 * @aiRequest The Chat request to send to the provider
	 * @callback A callback function to be called with each chunk of the stream: function( chunk )
	 *
	 * @return void
	 */
	function invokeStream( required AiRequest aiRequest, required function callback ){
		// Model Selection if not set, use the default in the service, which should always be set
		aiRequest
			.setModelIfEmpty( variables.params.model )
			.setApiKeyIfEmpty( getAPIKey() )
			.mergeServiceParams( variables.params )
			.mergeServiceHeaders( variables.headers )
			.setStream( true )
		// Do a chat stream request
		return chatStream( argumentCollection = arguments )
	}

	/**
	 * ---------------------------------------------------------------------------------------------------------
	 * Callers
	 * ---------------------------------------------------------------------------------------------------------
	 */

	/**
	 * A chat method that sends messages to the provider.
	 * This method in this base class is based of OpenAI's standard.
	 * If the provider does not support this standard, it should override this method.
	 *
	 * @aiRequest The AIRequest object to send to the provider
	 *
	 * @throws ProviderError if the provider returns an error from the request
	 *
	 * @return The response from the provider according to the return format in the AI request
	 */
	public function chat( required AiRequest aiRequest ){
		// Build the packet according to the OpenAI standard
		var dataPacket = {
			"model"   : arguments.aiRequest.getModel(),
			"messages": arguments.aiRequest.getMessages()
		}.append( arguments.aiRequest.getParams() )

		// Tooling support
		if( dataPacket.keyExists( "tools" ) ){
			dataPacket.tools = dataPacket.tools.map( .getSchema )
		}

		// Send it
		var result = sendRequest( aiRequest, dataPacket )

		// If an error is returned, throw it
		if( result.keyExists( "error" ) ){
			writeLog(
				text: result.error.toString(),
				type: "error",
				log : "ai"
			)
			throw(
				type   : "ProviderError",
				message: result.error.toString()
			);
		}

		// Result returns, only if we are not using tool calls
		if( !result.choices.first().message.keyExists( "tool_calls" ) ){
			// Determine return formats
			switch( aiRequest.getReturnFormat() ){
				case "all":
					return result.choices;
				case "raw":
					return result;
				case "single": default:
					return result.choices.first().message.content;
			}
		}

		/**
		 * ---------------------------------------------------------------------------------------------------------
		 * Tool Chains
		 * ---------------------------------------------------------------------------------------------------------
		 */
		var newMessages = aiRequest.getMessages().map( message -> message );
		result.choices.each( ( choice, i ) => {

			// add the tool call into our message history
			newMessages.append( choice.message );

			// find the tool, invoke it, append the result to the chat history
			choice.message.tool_calls.each( ( toolCall, i ) => {
				aiRequest.getTool( toolCall.function.name )
					.ifPresentOrElse(
						tool => {
							newMessages.append({
								"role"        : "tool",
								"tool_call_id": toolCall.id,
								"content"     : tool.invoke( JSONDeserialize( toolCall.function.arguments ) )
							});
						},
						() => {
							writeLog(
								text: "Unable to find tool named: #toolCall.function.name#",
								type: "warning",
								log : "ai"
							)
							newMessages.append({
								"role"        : "tool",
								"tool_call_id": toolCall.id,
								"content"     : "Tool ['#toolCall.function.name#'] not found in chat request"
							});
						}
					)
			});
		});

		aiRequest.setMessages( newMessages )

		return chat( aiRequest )
	}

	/**
	 * A chat stream method that sends messages to the provider and streams the response.
	 * This method in this base class is based of OpenAI's standard.
	 * If the provider does not support this standard, it should override this method.
	 *
	 * @aiRequest The aiRequest object to send to the provider
	 * @callback A callback function to be called with each chunk of the stream: function( chunk )
	 *
	 * @throws ProviderError if the provider returns an error from the request
	 *
	 * @return void
	 */
	public function chatStream( required AiRequest aiRequest, required function callback ){
		// Build the packet according to the OpenAI standard
		var dataPacket = {
			"model"   : arguments.aiRequest.getModel(),
			"messages": arguments.aiRequest.getMessages(),
			"stream"  : true
		}.append( arguments.aiRequest.getParams() )

		// Tooling support
		if( dataPacket.keyExists( "tools" ) ){
			dataPacket.tools = dataPacket.tools.map( .getSchema )
		}

		// Send it with streaming
		sendStreamRequest( aiRequest, dataPacket, callback )
	}

	/**
	 * ---------------------------------------------------------------------------------------------------------
	 * Private Methods
	 * ---------------------------------------------------------------------------------------------------------
	 */

	/**
	 * A generic HTTP proxy to send requests to the provider
	 *
	 * @aiRequest The aiRequest object to use in the request
	 * @dataPacket The data packet to send to the provider
	 *
	 * @return The response from the provider as a struct
	 */
	private function sendRequest( required AiRequest aiRequest, required struct dataPacket ){
		// Announce the request
		BoxAnnounce(
			"onAIRequest",
			{
				"dataPacket" : arguments.dataPacket,
				"aiRequest": arguments.aiRequest,
				"provider"   : this
			}
		);

		// Log the request
		if( arguments.aiRequest.getLogRequest() ){
			writeLog(
				text: "Request to AI Provider: #arguments.dataPacket.toString()#",
				type: "info",
				log : "ai"
			)
		}

		// Log the request to the console if enabled
		if( arguments.aiRequest.getLogRequestToConsole() ){
			println( "AI Request" )
			println( arguments.dataPacket )
		}

		bx: http
			url     = getChatURL()
			method  = "post"
			result  = "chatResult"
			charset = "utf-8"
			timeout = arguments.aiRequest.getTimeout()
		{
			bx:httpParam type="header" name="content-type" value="application/json";

			// Auth Header ONLY if the provider requires it
			// This is the default for OpenAI
			// If the provider does not require it, set it to false
			if( arguments.aiRequest.getSendAuthHeader() ){
				bx:httpParam type="header" name="Authorization" value="Bearer #arguments.aiRequest.getApiKey()#";
			}

			// Custom Headers
			for( var thisHeader in arguments.aiRequest.getHeaders() ){
				bx:httpParam
					type="header"
					name="#thisHeader#"
					value="#arguments.aiRequest.getHeaders()[ thisHeader ]#";
			}

			// Body Packet
			bx:httpParam type="body" value=jsonSerialize( arguments.dataPacket );
		}

		// Final logging if the provider supports it
		if( arguments.aiRequest.getLogResponse() ){
			writeLog(
				text: "Response from AI Provider: #chatResult.toString()#",
				type: "info",
				log : "ai"
			)
		}

		// Log the response to the console if enabled
		if( arguments.aiRequest.getLogResponseToConsole() ){
			println( "AI Response" )
			println( chatResult )
			println( "AI Deserialized Response" )
			println( jsonDeserialize( chatResult.filecontent ) )
		}

		var iData = {
			"aiRequest" = arguments.aiRequest,
			"response" = jsonDeserialize( chatResult.filecontent ),
			"rawResponse"	: chatResult,
			"provider" = this
		}

		BoxAnnounce( "onAIResponse", iData );

		return iData.response;
	}

	/**
	 * A generic HTTP proxy to send streaming requests to the provider
	 *
	 * @aiRequest The aiRequest object to use in the request
	 * @dataPacket The data packet to send to the provider
	 * @callback A callback function to be called with each chunk of the stream: function( chunk )
	 *
	 * @return void
	 */
	private function sendStreamRequest(
		required AiRequest aiRequest,
		required struct dataPacket,
		required function callback
	 ){
		// Announce the request
		BoxAnnounce(
			"onAIRequest",
			{
				"dataPacket" : arguments.dataPacket,
				"aiRequest": arguments.aiRequest,
				"provider"   : this
			}
		);

		// Log the request
		if( arguments.aiRequest.getLogRequest() ){
			writeLog(
				text: "Request to AI Provider: #arguments.dataPacket.toString()#",
				type: "info",
				log : "ai"
			)
		}

		// Log the request to the console if enabled
		if( arguments.aiRequest.getLogRequestToConsole() ){
			println( "AI Request (Stream)" )
			println( arguments.dataPacket )
		}

		bx: http
			url     = getChatURL()
			method  = "post"
			result  = "chatResult"
			charset = "utf-8"
			timeout = arguments.aiRequest.getTimeout()
		{
			bx:httpParam type="header" name="content-type" value="application/json";

			// Auth Header ONLY if the provider requires it
			// This is the default for OpenAI
			// If the provider does not require it, set it to false
			if( arguments.aiRequest.getSendAuthHeader() ){
				bx:httpParam type="header" name="Authorization" value="Bearer #arguments.aiRequest.getApiKey()#";
			}

			// Custom Headers
			for( var thisHeader in arguments.aiRequest.getHeaders() ){
				bx:httpParam
					type="header"
					name="#thisHeader#"
					value="#arguments.aiRequest.getHeaders()[ thisHeader ]#";
			}

			// Body Packet
			bx:httpParam type="body" value=jsonSerialize( arguments.dataPacket );
		}

		// Check for HTTP errors
		if( chatResult.statusCode >= 300 ){
			var errorMessage = "HTTP Error #chatResult.statusCode#";
			try {
				var errorData = jsonDeserialize( chatResult.filecontent );
				if( errorData.keyExists( "error" ) ){
					errorMessage = errorData.error.toString();
				}
			} catch( any e ){
				// If we can't parse the error, use the raw response
				errorMessage &= ": #chatResult.filecontent#";
			}

			writeLog(
				text: "Stream Request Error: #errorMessage#",
				type: "error",
				log : "ai"
			)

			throw(
				type   : "ProviderError",
				message: errorMessage
			);
		}

		// Process the streaming response
		var streamContent = chatResult.filecontent;

		// Log the response if enabled
		if( arguments.aiRequest.getLogResponse() ){
			writeLog(
				text: "Stream Response from AI Provider: #streamContent.toString()#",
				type: "info",
				log : "ai"
			)
		}

		// Parse and process Server-Sent Events (SSE) format
		// OpenAI and most providers use SSE format: "data: {json}\n\n"
		var lines = streamContent.split( "\n" );
		var buffer = "";

		for( var line in lines ){
			line = line.trim();

			// Skip empty lines
			if( line.isEmpty() ){
				continue;
			}

			// Check if it's a data line
			if( line.startsWith( "data: " ) ){
				var jsonData = line.substring( 6 ).trim();

				// Check for stream end marker
				if( jsonData == "[DONE]" ){
					break;
				}

				// Try to parse and send to callback
				try {
					var chunk = jsonDeserialize( jsonData );
					arguments.callback( chunk );
				} catch( any e ){
					writeLog(
						text: "Error parsing stream chunk: #e.message# - Data: #jsonData#",
						type: "error",
						log : "ai"
					)
				}
			}
		}

		BoxAnnounce(
			"onAIResponse",
			{
				"aiRequest" : arguments.aiRequest,
				"provider"    : this,
				"streamComplete" : true
			}
		);
	}

}
