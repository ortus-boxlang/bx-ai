// ollama-hello.bxs
/**
 * Using Ollama for Local AI
 * No API key required - runs on your machine!
 * 
 * Prerequisites:
 * 1. Install Ollama from https://ollama.ai
 * 2. Run: ollama pull llama3.2
 */

println( "=== Local AI with Ollama ===" )

// Use Ollama (local, free)
answer = aiChat(
    "Explain what a variable is in programming in one sentence",
    { model: "llama3.2" },
    { provider: "ollama" }
)

println( answer )
println()
println( "âœ… This ran locally on your machine - no API costs!" )
