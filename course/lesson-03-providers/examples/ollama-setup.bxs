// ollama-setup.bxs
/**
 * Ollama Setup and Testing
 * Guide to setting up and using Ollama
 */

println( "=== Ollama Setup Guide ===" )
println()

println( "Step 1: Check if Ollama is running" )
println( "-".repeat( 60 ) )

try {
    // Try to connect to Ollama
    answer = aiChat(
        "Say 'OK' if you're working",
        { model: "llama3.2" },
        { provider: "ollama" }
    )

    println( "‚úÖ Ollama is running!" )
    println( "Response: #answer#" )

} catch( any e ) {
    println( "‚ùå Ollama is not running or not configured" )
    println()
    println( "Setup instructions:" )
    println( "1. Install Ollama:" )
    println( "   macOS:   brew install ollama" )
    println( "   Linux:   curl -fsSL https://ollama.ai/install.sh | sh" )
    println( "   Windows: Download from https://ollama.ai" )
    println()
    println( "2. Start Ollama: ollama serve" )
    println()
    println( "3. Pull a model: ollama pull llama3.2" )
    println()
    abort;
}

println()
println( "Step 2: Available Models" )
println( "-".repeat( 60 ) )

// List recommended models
models = [
    { name: "llama3.2", size: "3B", use: "General chat, fast responses" },
    { name: "qwen2.5:0.5b-instruct", size: "0.5B", use: "Ultra-fast, testing" },
    { name: "mistral", size: "7B", use: "Technical content, code" },
    { name: "codellama", size: "7B", use: "Code generation" },
    { name: "phi", size: "2.7B", use: "Efficient, good quality" }
]

println( "Recommended models:" )
models.each( model => {
    println( "  ‚Ä¢ #model.name# (#model.size#) - #model.use#" )
} )

println()
println( "To pull a model: ollama pull <model-name>" )

println()
println( "Step 3: Test Multiple Models" )
println( "-".repeat( 60 ) )

testPrompt = "What is BoxLang?"
modelsToTest = [ "llama3.2", "qwen2.5:0.5b-instruct" ]

modelsToTest.each( modelName => {
    println()
    println( "Testing: #modelName#" )

    try {
        startTime = getTickCount()

        answer = aiChat(
            testPrompt,
            { model: modelName },
            { provider: "ollama" }
        )

        elapsed = getTickCount() - startTime

        println( "  Response: #answer.left( 100 )#..." )
        println( "  Time: #elapsed#ms" )
        println( "  ‚úÖ Model works!" )

    } catch( any e ) {
        println( "  ‚ùå Model not available" )
        println( "  Run: ollama pull #modelName#" )
    }
} )

println()
println( "üí° Ollama Benefits:" )
println( "- 100% FREE (no API costs)" )
println( "- Private (data never leaves your machine)" )
println( "- Fast (no network latency)" )
println( "- Offline capable" )
println( "- Perfect for development and testing" )
