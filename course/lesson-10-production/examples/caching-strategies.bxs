// caching-strategies.bxs
/**
 * Caching Strategies for AI Responses
 * Reduce costs and improve performance
 */

println( "=== Caching Strategies Demo ===" )
println()

// Simple in-memory cache
cache = {}

// Pattern 1: Response Caching
println( "Pattern 1: Simple Response Cache" )
println( "-".repeat( 70 ) )

function cachedChat( prompt, ttlMinutes = 60 ) {
    // Generate cache key
    cacheKey = hash( prompt )
    
    // Check cache
    if ( cache.keyExists( cacheKey ) ) {
        cached = cache[ cacheKey ]
        
        // Check if expired
        age = dateDiff( "n", cached.timestamp, now() )
        if ( age < ttlMinutes ) {
            println( "âœ“ Cache hit (age: #age# minutes)" )
            return cached.response
        } else {
            println( "âš ï¸  Cache expired" )
            cache.delete( cacheKey )
        }
    }
    
    println( "âœ— Cache miss - calling AI" )
    response = aiChat( prompt )
    
    // Store in cache
    cache[ cacheKey ] = {
        response: response,
        timestamp: now()
    }
    
    return response
}

// Test caching
prompts = [ "What is 2+2?", "What is 2+2?", "What is 3+3?" ]

prompts.each( ( prompt, index ) => {
    println( "Request #index#: #prompt#" )
    result = cachedChat( prompt, 5 )
    println( "Response: #result#" )
    println()
} )

// Pattern 2: Semantic Caching
println( "Pattern 2: Semantic Similarity Cache" )
println( "-".repeat( 70 ) )

// Cache with semantic matching
semanticCache = []

function semanticCachedChat( prompt, similarityThreshold = 0.8 ) {
    // Check for semantically similar prompts
    for ( entry in semanticCache ) {
        similarity = calculateSimilarity( prompt, entry.prompt )
        
        if ( similarity >= similarityThreshold ) {
            println( "âœ“ Semantic cache hit (similarity: #numberFormat( similarity, '0.00' )#)" )
            println( "  Original: #entry.prompt#" )
            println( "  Current:  #prompt#" )
            return entry.response
        }
    }
    
    println( "âœ— Semantic cache miss - calling AI" )
    response = aiChat( prompt )
    
    // Store in semantic cache
    semanticCache.append( {
        prompt: prompt,
        response: response,
        timestamp: now()
    } )
    
    return response
}

function calculateSimilarity( str1, str2 ) {
    // Simple Jaccard similarity (word overlap)
    words1 = str1.listToArray( " " )
    words2 = str2.listToArray( " " )
    
    intersection = words1.filter( w => words2.findNoCase( w ) > 0 ).len()
    union = words1.len() + words2.len() - intersection
    
    return union > 0 ? intersection / union : 0
}

// Test semantic caching
similarPrompts = [
    "What is artificial intelligence?",
    "Can you explain what AI is?",
    "Tell me about machine learning"
]

similarPrompts.each( prompt => {
    println( "Prompt: #prompt#" )
    result = semanticCachedChat( prompt, 0.3 )
    println()
} )

// Pattern 3: Partial Response Caching
println( "Pattern 3: Partial/Chunked Caching" )
println( "-".repeat( 70 ) )

function cachedPipeline( input ) {
    // Cache expensive preprocessing
    cacheKey = "preprocess_" & hash( input )
    
    if ( cache.keyExists( cacheKey ) ) {
        println( "âœ“ Using cached preprocessing" )
        preprocessed = cache[ cacheKey ]
    } else {
        println( "âœ— Running preprocessing" )
        preprocessed = input.uCase().trim()
        cache[ cacheKey ] = preprocessed
    }
    
    // Fresh AI call with preprocessed input
    return aiChat( "Process: #preprocessed#" )
}

result = cachedPipeline( "  hello world  " )
println( "Result: #result#" )
println()

result = cachedPipeline( "  hello world  " )  // Second call uses cache
println( "Result: #result#" )
println()

// Cache Statistics
println( "=== Cache Statistics ===" )
println( "Simple cache entries: #cache.count()#" )
println( "Semantic cache entries: #semanticCache.len()#" )
println()

println( "ðŸ’¡ Caching Best Practices:" )
println( "- Cache common/repeated queries" )
println( "- Set appropriate TTL" )
println( "- Use semantic matching for similar queries" )
println( "- Cache preprocessing steps" )
println( "- Monitor cache hit rates" )
println( "- Implement cache warming for common queries" )
println( "- Consider Redis/Memcached for distributed apps" )
