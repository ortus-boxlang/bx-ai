// monitoring.bxs
/**
 * Monitoring and Observability
 * Track AI usage, costs, and performance
 */

println( "=== AI Monitoring Demo ===" )
println()

// Metrics storage
metrics = {
    requests: [],
    totalCost: 0,
    totalTokens: 0,
    errors: []
}

// Monitored AI wrapper
function monitoredChat( prompt, options = {} ) {
    startTime = getTickCount()
    requestId = createUUID()
    
    try {
        // Make request
        response = aiChat( prompt, options )
        
        // Record metrics
        duration = getTickCount() - startTime
        tokens = estimateTokens( prompt, response )
        cost = estimateCost( tokens, options.model ?: "gpt-4o-mini" )
        
        metrics.requests.append( {
            id: requestId,
            timestamp: now(),
            prompt: left( prompt, 100 ),
            model: options.model ?: "gpt-4o-mini",
            tokens: tokens,
            cost: cost,
            duration: duration,
            status: "success"
        } )
        
        metrics.totalCost += cost
        metrics.totalTokens += tokens
        
        println( "âœ“ Request #requestId.left( 8 )# completed in #duration#ms (#tokens# tokens, $#numberFormat( cost, '0.0000' )#)" )
        
        return response
        
    } catch( any e ) {
        duration = getTickCount() - startTime
        
        // Record error
        metrics.errors.append( {
            id: requestId,
            timestamp: now(),
            error: e.message,
            duration: duration
        } )
        
        println( "âŒ Request #requestId.left( 8 )# failed after #duration#ms: #e.message#" )
        throw e
    }
}

function estimateTokens( prompt, response ) {
    // Rough estimation: ~4 chars per token
    return ( len( prompt ) + len( response ) ) / 4
}

function estimateCost( tokens, model ) {
    // Rough cost estimates per 1K tokens
    costs = {
        "gpt-4o-mini": 0.00015,
        "gpt-4o": 0.0025,
        "gpt-3.5-turbo": 0.0005
    }
    
    rate = costs[ model ] ?: 0.0001
    return ( tokens / 1000 ) * rate
}

// Test monitoring
println( "Running monitored requests..." )
println( "-".repeat( 70 ) )
println()

prompts = [
    "What is 2+2?",
    "Explain quantum physics in one sentence",
    "Write a haiku about programming"
]

prompts.each( prompt => {
    try {
        result = monitoredChat( prompt )
        println( "Response: #left( result, 50 )#..." )
    } catch( any e ) {
        println( "Failed: #prompt#" )
    }
    println()
} )

// Generate report
println( "=== Monitoring Report ===" )
println( "-".repeat( 70 ) )
println()

println( "Total Requests: #metrics.requests.len()#" )
println( "Total Errors: #metrics.errors.len()#" )
println( "Success Rate: #numberFormat( ( metrics.requests.len() / ( metrics.requests.len() + metrics.errors.len() ) ) * 100, '0.00' )#%" )
println()

println( "Token Usage: #numberFormat( metrics.totalTokens, '0,0' )# tokens" )
println( "Total Cost: $#numberFormat( metrics.totalCost, '0.0000' )#" )
println()

// Average performance
if ( metrics.requests.len() > 0 ) {
    avgDuration = metrics.requests.reduce( ( sum, req ) => sum + req.duration, 0 ) / metrics.requests.len()
    avgTokens = metrics.totalTokens / metrics.requests.len()
    avgCost = metrics.totalCost / metrics.requests.len()
    
    println( "Average Performance:" )
    println( "  Duration: #numberFormat( avgDuration, '0' )#ms" )
    println( "  Tokens: #numberFormat( avgTokens, '0' )# per request" )
    println( "  Cost: $#numberFormat( avgCost, '0.0000' )# per request" )
    println()
}

// Model breakdown
println( "By Model:" )
modelStats = {}
metrics.requests.each( req => {
    if ( !modelStats.keyExists( req.model ) ) {
        modelStats[ req.model ] = { count: 0, tokens: 0, cost: 0 }
    }
    modelStats[ req.model ].count++
    modelStats[ req.model ].tokens += req.tokens
    modelStats[ req.model ].cost += req.cost
} )

modelStats.each( ( model, stats ) => {
    println( "  #model#:" )
    println( "    Requests: #stats.count#" )
    println( "    Tokens: #numberFormat( stats.tokens, '0,0' )#" )
    println( "    Cost: $#numberFormat( stats.cost, '0.0000' )#" )
} )
println()

// Recent errors
if ( metrics.errors.len() > 0 ) {
    println( "Recent Errors:" )
    metrics.errors.each( error => {
        println( "  [#dateTimeFormat( error.timestamp, 'HH:mm:ss' )#] #error.error#" )
    } )
    println()
}

println( "ðŸ’¡ Monitoring Best Practices:" )
println( "- Track all requests and responses" )
println( "- Monitor token usage and costs" )
println( "- Log errors and failures" )
println( "- Measure performance (latency)" )
println( "- Set up alerts for anomalies" )
println( "- Export metrics to monitoring system" )
println( "- Generate regular reports" )
println( "- Monitor rate limits" )
