// document-pipeline.bxs
/**
 * LAB: Document Processing Pipeline
 * 
 * OBJECTIVE:
 * Build a pipeline that processes documents through multiple AI stages
 * 
 * REQUIREMENTS:
 * 1. Accept document input (text)
 * 2. Classify document type (email, report, note, etc.)
 * 3. Extract key information (dates, people, actions)
 * 4. Generate summary
 * 5. Output structured result
 * 
 * BONUS CHALLENGES:
 * - Handle multiple documents in batch
 * - Add priority/urgency detection
 * - Extract and validate metadata
 * - Generate different summaries for different audiences
 * - Create pipeline visualization (show stages)
 * - Add error handling and retries
 * - Cache intermediate results
 * 
 * EXAMPLE OUTPUT:
 * Processing document...
 * Stage 1: Classification -> "email"
 * Stage 2: Extraction -> {date: "2024-01-15", person: "John"}
 * Stage 3: Summary -> "Meeting request for..."
 * 
 * Result: {
 *   type: "email",
 *   metadata: {...},
 *   summary: "...",
 *   priority: "normal"
 * }
 * 
 * HINTS:
 * - Use aiModel() for AI stages
 * - Use aiTransform() for data transformations
 * - Chain with .to()
 * - Pass data between stages using structs
 */

println( "=== Document Processing Pipeline Lab ===" )
println()

// Sample documents to process
documents = [
    "Email from boss: Team meeting tomorrow at 2 PM to discuss Q1 goals",
    "Note: Remember to submit expense report by Friday",
    "Report: Sales increased 15% this quarter, reaching $2M in revenue"
]

// YOUR CODE HERE
//
// Step 1: Create classification stage
// classifier = aiTransform( ( doc ) => {
//     type = aiChat( "Classify as email/note/report: #doc#" )
//     return { original: doc, type: type }
// } )
//
// Step 2: Create extraction stage
// extractor = aiTransform( ( data ) => {
//     info = aiChat( "Extract key info as JSON: #data.original#" )
//     data.extracted = info
//     return data
// } )
//
// Step 3: Create summary stage
// summarizer = aiTransform( ( data ) => {
//     summary = aiChat( "Summarize in 10 words: #data.original#" )
//     data.summary = summary
//     return data
// } )
//
// Step 4: Chain into pipeline
// pipeline = classifier.to( extractor ).to( summarizer )
//
// Step 5: Process documents
// documents.each( doc => {
//     result = pipeline.run( doc )
//     println( serializeJSON( result, true ) )
// } )

println( "ðŸ’¡ Implement your document pipeline above!" )
println()
println( "Test with provided documents array" )
